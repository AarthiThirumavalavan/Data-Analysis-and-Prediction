{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***ASSIGNMENT SOLUTION -  AARTHI THIRUMAVALAVAN***\n",
        "\n",
        "Contents:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Imports\n",
        "*   Reading Input Data\n",
        "*   Data Pre-processing\n",
        "*   Essential Functions\n",
        "*   Train-test split for model input\n",
        "*   Model design - Baseline\n",
        "*   SMOTE AND SMOTETOMEK sampling strategy on classifiers\n",
        "*   One vs Rest dataset split on Classifiers\n",
        "*   Best Model - Catboost with SMOTE sampling strategy and One-vs-Rest dataset split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sGecmBKZMDpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "kfDugfKQu7FJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyaEEtO-U2x1",
        "outputId": "0c8432c5-bcf0-4904-cb7f-8248cfeb2800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.4-cp37-none-manylinux1_x86_64.whl (76.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.4\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef as mcc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "!pip install catboost\n",
        "import catboost\n",
        "import matplotlib.pyplot as plt\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwn7FfWoU6lj",
        "outputId": "c7c8a113-5ac0-45fd-99b2-170a6884c0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading input data"
      ],
      "metadata": {
        "id": "csQiQrbavBaj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W_wdWgjU2x6"
      },
      "outputs": [],
      "source": [
        "df_main = pd.read_csv('/content/drive/MyDrive/Micron/Q1_data.csv')\n",
        "df = df_main.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jStd6qNXU2x7",
        "outputId": "ce8bf0a6-46ca-4083-93c1-e4201a6ba21d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10241, 152)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing"
      ],
      "metadata": {
        "id": "guPRrIydvFx5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAbEvG4eU2x8"
      },
      "outputs": [],
      "source": [
        "#To check for duplicate values\n",
        "df.drop_duplicates(keep=\"first\",inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJsYpUQdU2x8",
        "outputId": "bec64d98-fadf-4f4f-fb22-48c119c28568"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10241, 152)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.shape #proves no duplicate rows are present"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTvYX9yAginc",
        "outputId": "10f3469b-fd53-4de5-dd62-f94208f89f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 10241 entries, 0 to 10240\n",
            "Columns: 152 entries, 0 to target\n",
            "dtypes: float64(143), int64(9)\n",
            "memory usage: 12.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To check for null values\n",
        "df.isnull().any().value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOQ2IMlxgnG3",
        "outputId": "439e66e5-96dd-486c-efc6-9ad7ea00073b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    147\n",
              "True       5\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(method=\"ffill\",inplace=True)"
      ],
      "metadata": {
        "id": "0fOnCHENg1H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6vSuZfOgroD",
        "outputId": "37b5f620-c3b1-4d42-fde5-f3b8cc1a911c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    10000\n",
              "1      101\n",
              "2       59\n",
              "3       52\n",
              "4       29\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Oh-3ZOD_U2x9",
        "outputId": "55049450-e7fe-4bee-dc18-a37552ea1f5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f58e9bc1c50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Target distribution in dataset\n",
        "df['target'].value_counts(normalize=True).plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoNSK7KwU2x-",
        "outputId": "588c4b32-b19e-4663-c57d-9e06b43ee4f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float64    143\n",
              "int64        9\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Distribution of float and int datatypes in the dataset\n",
        "df.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization and Standardization of dataset**"
      ],
      "metadata": {
        "id": "e1klPsge4gZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "KTbeda3jU2yA",
        "outputId": "62e81c70-fb96-4b77-9086-157c93b7b1a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5c7624fe-05f4-400f-b632-3caa37fcd577\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.101724</td>\n",
              "      <td>0.029776</td>\n",
              "      <td>0.062929</td>\n",
              "      <td>0.623310</td>\n",
              "      <td>0.021386</td>\n",
              "      <td>0.022632</td>\n",
              "      <td>0.019447</td>\n",
              "      <td>0.621639</td>\n",
              "      <td>0.025841</td>\n",
              "      <td>0.260170</td>\n",
              "      <td>0.064564</td>\n",
              "      <td>0.075057</td>\n",
              "      <td>0.003672</td>\n",
              "      <td>0.536461</td>\n",
              "      <td>0.031011</td>\n",
              "      <td>0.457383</td>\n",
              "      <td>0.042836</td>\n",
              "      <td>0.022252</td>\n",
              "      <td>0.224006</td>\n",
              "      <td>0.116258</td>\n",
              "      <td>0.551649</td>\n",
              "      <td>0.263242</td>\n",
              "      <td>0.130858</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.175314</td>\n",
              "      <td>0.317520</td>\n",
              "      <td>0.616557</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.074161</td>\n",
              "      <td>0.065702</td>\n",
              "      <td>0.013334</td>\n",
              "      <td>0.038237</td>\n",
              "      <td>0.471972</td>\n",
              "      <td>0.134564</td>\n",
              "      <td>0.067579</td>\n",
              "      <td>0.117843</td>\n",
              "      <td>0.139478</td>\n",
              "      <td>0.026804</td>\n",
              "      <td>0.503864</td>\n",
              "      <td>0.013739</td>\n",
              "      <td>...</td>\n",
              "      <td>0.679008</td>\n",
              "      <td>0.190292</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.026366</td>\n",
              "      <td>0.008412</td>\n",
              "      <td>0.904658</td>\n",
              "      <td>0.019225</td>\n",
              "      <td>0.793347</td>\n",
              "      <td>0.223521</td>\n",
              "      <td>0.086735</td>\n",
              "      <td>0.931911</td>\n",
              "      <td>0.260225</td>\n",
              "      <td>0.514515</td>\n",
              "      <td>0.310123</td>\n",
              "      <td>0.207216</td>\n",
              "      <td>0.320051</td>\n",
              "      <td>0.048744</td>\n",
              "      <td>0.772251</td>\n",
              "      <td>0.908139</td>\n",
              "      <td>0.598463</td>\n",
              "      <td>0.260550</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.013346</td>\n",
              "      <td>0.652996</td>\n",
              "      <td>0.015334</td>\n",
              "      <td>0.227611</td>\n",
              "      <td>0.076653</td>\n",
              "      <td>0.577633</td>\n",
              "      <td>0.072140</td>\n",
              "      <td>0.461526</td>\n",
              "      <td>0.446814</td>\n",
              "      <td>0.645971</td>\n",
              "      <td>0.039490</td>\n",
              "      <td>0.575194</td>\n",
              "      <td>0.850136</td>\n",
              "      <td>0.183208</td>\n",
              "      <td>0.840474</td>\n",
              "      <td>0.413013</td>\n",
              "      <td>0.448460</td>\n",
              "      <td>0.201133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.104126</td>\n",
              "      <td>0.016637</td>\n",
              "      <td>0.054497</td>\n",
              "      <td>0.343901</td>\n",
              "      <td>0.025502</td>\n",
              "      <td>0.025125</td>\n",
              "      <td>0.149282</td>\n",
              "      <td>0.363075</td>\n",
              "      <td>0.024354</td>\n",
              "      <td>0.273569</td>\n",
              "      <td>0.066245</td>\n",
              "      <td>0.263259</td>\n",
              "      <td>0.003672</td>\n",
              "      <td>0.543657</td>\n",
              "      <td>0.027433</td>\n",
              "      <td>0.108321</td>\n",
              "      <td>0.030970</td>\n",
              "      <td>0.023432</td>\n",
              "      <td>0.304639</td>\n",
              "      <td>0.738191</td>\n",
              "      <td>0.851835</td>\n",
              "      <td>0.173169</td>\n",
              "      <td>0.085524</td>\n",
              "      <td>0.000554</td>\n",
              "      <td>0.546985</td>\n",
              "      <td>0.142946</td>\n",
              "      <td>0.614138</td>\n",
              "      <td>0.051963</td>\n",
              "      <td>0.078354</td>\n",
              "      <td>0.027745</td>\n",
              "      <td>0.013826</td>\n",
              "      <td>0.044039</td>\n",
              "      <td>0.244787</td>\n",
              "      <td>0.035119</td>\n",
              "      <td>0.067406</td>\n",
              "      <td>0.391294</td>\n",
              "      <td>0.128158</td>\n",
              "      <td>0.026320</td>\n",
              "      <td>0.482547</td>\n",
              "      <td>0.020822</td>\n",
              "      <td>...</td>\n",
              "      <td>0.714569</td>\n",
              "      <td>0.516983</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.021263</td>\n",
              "      <td>0.013488</td>\n",
              "      <td>0.249272</td>\n",
              "      <td>0.017707</td>\n",
              "      <td>0.356519</td>\n",
              "      <td>0.211925</td>\n",
              "      <td>0.144770</td>\n",
              "      <td>0.635637</td>\n",
              "      <td>0.020330</td>\n",
              "      <td>0.317805</td>\n",
              "      <td>0.271594</td>\n",
              "      <td>0.232141</td>\n",
              "      <td>0.852692</td>\n",
              "      <td>0.317969</td>\n",
              "      <td>0.819605</td>\n",
              "      <td>0.589945</td>\n",
              "      <td>0.668468</td>\n",
              "      <td>0.667604</td>\n",
              "      <td>0.466591</td>\n",
              "      <td>0.012154</td>\n",
              "      <td>0.652014</td>\n",
              "      <td>0.007119</td>\n",
              "      <td>0.334974</td>\n",
              "      <td>0.111568</td>\n",
              "      <td>0.635343</td>\n",
              "      <td>0.033274</td>\n",
              "      <td>0.466487</td>\n",
              "      <td>0.423277</td>\n",
              "      <td>0.465174</td>\n",
              "      <td>0.034991</td>\n",
              "      <td>0.320416</td>\n",
              "      <td>0.865218</td>\n",
              "      <td>0.234139</td>\n",
              "      <td>0.694571</td>\n",
              "      <td>0.395686</td>\n",
              "      <td>0.272417</td>\n",
              "      <td>0.195889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.068141</td>\n",
              "      <td>0.025432</td>\n",
              "      <td>0.055903</td>\n",
              "      <td>0.635276</td>\n",
              "      <td>0.021586</td>\n",
              "      <td>0.021320</td>\n",
              "      <td>0.018033</td>\n",
              "      <td>0.502120</td>\n",
              "      <td>0.021379</td>\n",
              "      <td>0.135659</td>\n",
              "      <td>0.182025</td>\n",
              "      <td>0.090951</td>\n",
              "      <td>0.004896</td>\n",
              "      <td>0.562581</td>\n",
              "      <td>0.024571</td>\n",
              "      <td>0.338656</td>\n",
              "      <td>0.040810</td>\n",
              "      <td>0.023601</td>\n",
              "      <td>0.137171</td>\n",
              "      <td>0.071036</td>\n",
              "      <td>0.811086</td>\n",
              "      <td>0.255188</td>\n",
              "      <td>0.140021</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.339515</td>\n",
              "      <td>0.269663</td>\n",
              "      <td>0.122441</td>\n",
              "      <td>0.035589</td>\n",
              "      <td>0.069182</td>\n",
              "      <td>0.060596</td>\n",
              "      <td>0.011710</td>\n",
              "      <td>0.044072</td>\n",
              "      <td>0.610675</td>\n",
              "      <td>0.129188</td>\n",
              "      <td>0.062727</td>\n",
              "      <td>0.070730</td>\n",
              "      <td>0.356782</td>\n",
              "      <td>0.025997</td>\n",
              "      <td>0.568079</td>\n",
              "      <td>0.009986</td>\n",
              "      <td>...</td>\n",
              "      <td>0.483446</td>\n",
              "      <td>0.354507</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.018711</td>\n",
              "      <td>0.011603</td>\n",
              "      <td>0.419214</td>\n",
              "      <td>0.009511</td>\n",
              "      <td>0.767809</td>\n",
              "      <td>0.249026</td>\n",
              "      <td>0.153699</td>\n",
              "      <td>0.436687</td>\n",
              "      <td>0.017819</td>\n",
              "      <td>0.412497</td>\n",
              "      <td>0.286358</td>\n",
              "      <td>0.122407</td>\n",
              "      <td>0.134167</td>\n",
              "      <td>0.240940</td>\n",
              "      <td>0.821727</td>\n",
              "      <td>0.239697</td>\n",
              "      <td>0.242020</td>\n",
              "      <td>0.612422</td>\n",
              "      <td>0.439105</td>\n",
              "      <td>0.013346</td>\n",
              "      <td>0.664784</td>\n",
              "      <td>0.009858</td>\n",
              "      <td>0.912726</td>\n",
              "      <td>0.077348</td>\n",
              "      <td>0.488141</td>\n",
              "      <td>0.074594</td>\n",
              "      <td>0.394753</td>\n",
              "      <td>0.674122</td>\n",
              "      <td>0.557913</td>\n",
              "      <td>0.031992</td>\n",
              "      <td>0.472277</td>\n",
              "      <td>0.686535</td>\n",
              "      <td>0.161211</td>\n",
              "      <td>0.773346</td>\n",
              "      <td>0.358557</td>\n",
              "      <td>0.327330</td>\n",
              "      <td>0.233047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.070115</td>\n",
              "      <td>0.033591</td>\n",
              "      <td>0.373985</td>\n",
              "      <td>0.801243</td>\n",
              "      <td>0.027912</td>\n",
              "      <td>0.016990</td>\n",
              "      <td>0.020720</td>\n",
              "      <td>0.298736</td>\n",
              "      <td>0.026957</td>\n",
              "      <td>0.175572</td>\n",
              "      <td>0.078802</td>\n",
              "      <td>0.070634</td>\n",
              "      <td>0.004896</td>\n",
              "      <td>0.534227</td>\n",
              "      <td>0.376670</td>\n",
              "      <td>0.399209</td>\n",
              "      <td>0.042981</td>\n",
              "      <td>0.024612</td>\n",
              "      <td>0.155472</td>\n",
              "      <td>0.487917</td>\n",
              "      <td>0.849576</td>\n",
              "      <td>0.169384</td>\n",
              "      <td>0.040592</td>\n",
              "      <td>0.012817</td>\n",
              "      <td>0.547577</td>\n",
              "      <td>0.047233</td>\n",
              "      <td>0.227148</td>\n",
              "      <td>0.025397</td>\n",
              "      <td>0.071017</td>\n",
              "      <td>0.065191</td>\n",
              "      <td>0.011415</td>\n",
              "      <td>0.040759</td>\n",
              "      <td>0.532141</td>\n",
              "      <td>0.134026</td>\n",
              "      <td>0.054930</td>\n",
              "      <td>0.036539</td>\n",
              "      <td>0.508389</td>\n",
              "      <td>0.027127</td>\n",
              "      <td>0.707034</td>\n",
              "      <td>0.027479</td>\n",
              "      <td>...</td>\n",
              "      <td>0.415822</td>\n",
              "      <td>0.444504</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.028280</td>\n",
              "      <td>0.015518</td>\n",
              "      <td>0.653930</td>\n",
              "      <td>0.014570</td>\n",
              "      <td>0.269153</td>\n",
              "      <td>0.433383</td>\n",
              "      <td>0.135204</td>\n",
              "      <td>0.720156</td>\n",
              "      <td>0.013992</td>\n",
              "      <td>0.335776</td>\n",
              "      <td>0.302751</td>\n",
              "      <td>0.164837</td>\n",
              "      <td>0.845909</td>\n",
              "      <td>0.191943</td>\n",
              "      <td>0.817880</td>\n",
              "      <td>0.783291</td>\n",
              "      <td>0.788127</td>\n",
              "      <td>0.443627</td>\n",
              "      <td>0.460146</td>\n",
              "      <td>0.012631</td>\n",
              "      <td>0.662328</td>\n",
              "      <td>0.020263</td>\n",
              "      <td>0.842903</td>\n",
              "      <td>0.074325</td>\n",
              "      <td>0.455045</td>\n",
              "      <td>0.071321</td>\n",
              "      <td>0.315949</td>\n",
              "      <td>0.565150</td>\n",
              "      <td>0.520474</td>\n",
              "      <td>0.038740</td>\n",
              "      <td>0.298968</td>\n",
              "      <td>0.264791</td>\n",
              "      <td>0.153224</td>\n",
              "      <td>0.649753</td>\n",
              "      <td>0.748939</td>\n",
              "      <td>0.270657</td>\n",
              "      <td>0.359184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.101594</td>\n",
              "      <td>0.023630</td>\n",
              "      <td>0.375390</td>\n",
              "      <td>0.342968</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.026371</td>\n",
              "      <td>0.024468</td>\n",
              "      <td>0.302509</td>\n",
              "      <td>0.022867</td>\n",
              "      <td>0.163605</td>\n",
              "      <td>0.169270</td>\n",
              "      <td>0.270279</td>\n",
              "      <td>0.003672</td>\n",
              "      <td>0.529468</td>\n",
              "      <td>0.029580</td>\n",
              "      <td>0.120079</td>\n",
              "      <td>0.011505</td>\n",
              "      <td>0.024612</td>\n",
              "      <td>0.215852</td>\n",
              "      <td>0.897473</td>\n",
              "      <td>0.668672</td>\n",
              "      <td>0.192851</td>\n",
              "      <td>0.038823</td>\n",
              "      <td>0.012757</td>\n",
              "      <td>0.561057</td>\n",
              "      <td>0.317520</td>\n",
              "      <td>0.526681</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>0.078092</td>\n",
              "      <td>0.027064</td>\n",
              "      <td>0.013432</td>\n",
              "      <td>0.036793</td>\n",
              "      <td>0.437345</td>\n",
              "      <td>0.215194</td>\n",
              "      <td>0.067753</td>\n",
              "      <td>0.362651</td>\n",
              "      <td>0.532242</td>\n",
              "      <td>0.031326</td>\n",
              "      <td>0.514122</td>\n",
              "      <td>0.021813</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600418</td>\n",
              "      <td>0.581171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024240</td>\n",
              "      <td>0.014068</td>\n",
              "      <td>0.711426</td>\n",
              "      <td>0.020136</td>\n",
              "      <td>0.361559</td>\n",
              "      <td>0.199717</td>\n",
              "      <td>0.113520</td>\n",
              "      <td>0.470129</td>\n",
              "      <td>0.026668</td>\n",
              "      <td>0.474150</td>\n",
              "      <td>0.265004</td>\n",
              "      <td>0.092563</td>\n",
              "      <td>0.362442</td>\n",
              "      <td>0.199015</td>\n",
              "      <td>0.823186</td>\n",
              "      <td>0.288384</td>\n",
              "      <td>0.520942</td>\n",
              "      <td>0.487557</td>\n",
              "      <td>0.509146</td>\n",
              "      <td>0.013108</td>\n",
              "      <td>0.667976</td>\n",
              "      <td>0.003012</td>\n",
              "      <td>0.661357</td>\n",
              "      <td>0.063042</td>\n",
              "      <td>0.489396</td>\n",
              "      <td>0.244375</td>\n",
              "      <td>0.531404</td>\n",
              "      <td>0.694408</td>\n",
              "      <td>0.375400</td>\n",
              "      <td>0.034491</td>\n",
              "      <td>0.266222</td>\n",
              "      <td>0.593548</td>\n",
              "      <td>0.087375</td>\n",
              "      <td>0.861007</td>\n",
              "      <td>0.454031</td>\n",
              "      <td>0.248016</td>\n",
              "      <td>0.251950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10236</th>\n",
              "      <td>0.087833</td>\n",
              "      <td>0.020451</td>\n",
              "      <td>0.056059</td>\n",
              "      <td>0.304429</td>\n",
              "      <td>0.018373</td>\n",
              "      <td>0.028077</td>\n",
              "      <td>0.018740</td>\n",
              "      <td>0.535040</td>\n",
              "      <td>0.021937</td>\n",
              "      <td>0.206849</td>\n",
              "      <td>0.173621</td>\n",
              "      <td>0.147387</td>\n",
              "      <td>0.007344</td>\n",
              "      <td>0.475089</td>\n",
              "      <td>0.028268</td>\n",
              "      <td>0.137246</td>\n",
              "      <td>0.042041</td>\n",
              "      <td>0.018712</td>\n",
              "      <td>0.180207</td>\n",
              "      <td>0.328634</td>\n",
              "      <td>0.744700</td>\n",
              "      <td>0.215031</td>\n",
              "      <td>0.037216</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.598869</td>\n",
              "      <td>0.380774</td>\n",
              "      <td>0.258423</td>\n",
              "      <td>0.026399</td>\n",
              "      <td>0.069838</td>\n",
              "      <td>0.064511</td>\n",
              "      <td>0.014958</td>\n",
              "      <td>0.038323</td>\n",
              "      <td>0.227568</td>\n",
              "      <td>0.133309</td>\n",
              "      <td>0.069659</td>\n",
              "      <td>0.142412</td>\n",
              "      <td>0.140893</td>\n",
              "      <td>0.026643</td>\n",
              "      <td>0.454303</td>\n",
              "      <td>0.026912</td>\n",
              "      <td>...</td>\n",
              "      <td>0.468407</td>\n",
              "      <td>0.572078</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.024878</td>\n",
              "      <td>0.008992</td>\n",
              "      <td>0.185590</td>\n",
              "      <td>0.019528</td>\n",
              "      <td>0.350806</td>\n",
              "      <td>0.245549</td>\n",
              "      <td>0.157526</td>\n",
              "      <td>0.775126</td>\n",
              "      <td>0.025592</td>\n",
              "      <td>0.427703</td>\n",
              "      <td>0.237178</td>\n",
              "      <td>0.209135</td>\n",
              "      <td>0.840610</td>\n",
              "      <td>0.109736</td>\n",
              "      <td>0.822390</td>\n",
              "      <td>0.737875</td>\n",
              "      <td>0.285931</td>\n",
              "      <td>0.544254</td>\n",
              "      <td>0.529049</td>\n",
              "      <td>0.007626</td>\n",
              "      <td>0.666257</td>\n",
              "      <td>0.013691</td>\n",
              "      <td>0.111658</td>\n",
              "      <td>0.096708</td>\n",
              "      <td>0.435988</td>\n",
              "      <td>0.071867</td>\n",
              "      <td>0.459910</td>\n",
              "      <td>0.362159</td>\n",
              "      <td>0.343031</td>\n",
              "      <td>0.027993</td>\n",
              "      <td>0.389015</td>\n",
              "      <td>0.728701</td>\n",
              "      <td>0.099441</td>\n",
              "      <td>0.785785</td>\n",
              "      <td>0.598303</td>\n",
              "      <td>0.372009</td>\n",
              "      <td>0.229196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10237</th>\n",
              "      <td>0.082773</td>\n",
              "      <td>0.020451</td>\n",
              "      <td>0.056059</td>\n",
              "      <td>0.304429</td>\n",
              "      <td>0.018373</td>\n",
              "      <td>0.028077</td>\n",
              "      <td>0.018740</td>\n",
              "      <td>0.618260</td>\n",
              "      <td>0.021937</td>\n",
              "      <td>0.226358</td>\n",
              "      <td>0.173621</td>\n",
              "      <td>0.106442</td>\n",
              "      <td>0.012240</td>\n",
              "      <td>0.531832</td>\n",
              "      <td>0.028268</td>\n",
              "      <td>0.163209</td>\n",
              "      <td>0.042041</td>\n",
              "      <td>0.018712</td>\n",
              "      <td>0.115562</td>\n",
              "      <td>0.121567</td>\n",
              "      <td>0.686203</td>\n",
              "      <td>0.292678</td>\n",
              "      <td>0.237360</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.573815</td>\n",
              "      <td>0.269663</td>\n",
              "      <td>0.258423</td>\n",
              "      <td>0.026399</td>\n",
              "      <td>0.069838</td>\n",
              "      <td>0.064511</td>\n",
              "      <td>0.014958</td>\n",
              "      <td>0.036974</td>\n",
              "      <td>0.279989</td>\n",
              "      <td>0.133309</td>\n",
              "      <td>0.069659</td>\n",
              "      <td>0.130212</td>\n",
              "      <td>0.374166</td>\n",
              "      <td>0.026643</td>\n",
              "      <td>0.729283</td>\n",
              "      <td>0.026912</td>\n",
              "      <td>...</td>\n",
              "      <td>0.378016</td>\n",
              "      <td>0.715566</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.024878</td>\n",
              "      <td>0.008992</td>\n",
              "      <td>0.851892</td>\n",
              "      <td>0.019528</td>\n",
              "      <td>0.696909</td>\n",
              "      <td>0.182523</td>\n",
              "      <td>0.084821</td>\n",
              "      <td>0.932202</td>\n",
              "      <td>0.025592</td>\n",
              "      <td>0.397982</td>\n",
              "      <td>0.245317</td>\n",
              "      <td>0.198703</td>\n",
              "      <td>0.832556</td>\n",
              "      <td>0.123879</td>\n",
              "      <td>0.819870</td>\n",
              "      <td>0.595271</td>\n",
              "      <td>0.575747</td>\n",
              "      <td>0.521316</td>\n",
              "      <td>0.609610</td>\n",
              "      <td>0.007626</td>\n",
              "      <td>0.642927</td>\n",
              "      <td>0.013691</td>\n",
              "      <td>0.197549</td>\n",
              "      <td>0.093035</td>\n",
              "      <td>0.561384</td>\n",
              "      <td>0.071867</td>\n",
              "      <td>0.473065</td>\n",
              "      <td>0.555137</td>\n",
              "      <td>0.353951</td>\n",
              "      <td>0.027993</td>\n",
              "      <td>0.483422</td>\n",
              "      <td>0.420019</td>\n",
              "      <td>0.087545</td>\n",
              "      <td>0.115696</td>\n",
              "      <td>0.611740</td>\n",
              "      <td>0.437244</td>\n",
              "      <td>0.149150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10238</th>\n",
              "      <td>0.088552</td>\n",
              "      <td>0.020451</td>\n",
              "      <td>0.056059</td>\n",
              "      <td>0.304429</td>\n",
              "      <td>0.018373</td>\n",
              "      <td>0.028077</td>\n",
              "      <td>0.018740</td>\n",
              "      <td>0.425622</td>\n",
              "      <td>0.021937</td>\n",
              "      <td>0.158399</td>\n",
              "      <td>0.173621</td>\n",
              "      <td>0.197521</td>\n",
              "      <td>0.001224</td>\n",
              "      <td>0.652597</td>\n",
              "      <td>0.630487</td>\n",
              "      <td>0.095126</td>\n",
              "      <td>0.042041</td>\n",
              "      <td>0.018712</td>\n",
              "      <td>0.125631</td>\n",
              "      <td>0.483889</td>\n",
              "      <td>0.871699</td>\n",
              "      <td>0.135949</td>\n",
              "      <td>0.332369</td>\n",
              "      <td>0.001489</td>\n",
              "      <td>0.775564</td>\n",
              "      <td>0.285685</td>\n",
              "      <td>0.780348</td>\n",
              "      <td>0.026399</td>\n",
              "      <td>0.069838</td>\n",
              "      <td>0.064511</td>\n",
              "      <td>0.014958</td>\n",
              "      <td>0.038137</td>\n",
              "      <td>0.481156</td>\n",
              "      <td>0.133309</td>\n",
              "      <td>0.069659</td>\n",
              "      <td>0.127252</td>\n",
              "      <td>0.374166</td>\n",
              "      <td>0.026643</td>\n",
              "      <td>0.454303</td>\n",
              "      <td>0.374858</td>\n",
              "      <td>...</td>\n",
              "      <td>0.564073</td>\n",
              "      <td>0.455871</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.024878</td>\n",
              "      <td>0.008992</td>\n",
              "      <td>0.625182</td>\n",
              "      <td>0.019528</td>\n",
              "      <td>0.235551</td>\n",
              "      <td>0.120109</td>\n",
              "      <td>0.162628</td>\n",
              "      <td>0.408220</td>\n",
              "      <td>0.025592</td>\n",
              "      <td>0.397982</td>\n",
              "      <td>0.252625</td>\n",
              "      <td>0.107948</td>\n",
              "      <td>0.790589</td>\n",
              "      <td>0.556888</td>\n",
              "      <td>0.828359</td>\n",
              "      <td>0.794318</td>\n",
              "      <td>0.098885</td>\n",
              "      <td>0.895261</td>\n",
              "      <td>0.437399</td>\n",
              "      <td>0.007626</td>\n",
              "      <td>0.677554</td>\n",
              "      <td>0.013691</td>\n",
              "      <td>0.498165</td>\n",
              "      <td>0.075773</td>\n",
              "      <td>0.408686</td>\n",
              "      <td>0.071867</td>\n",
              "      <td>0.442235</td>\n",
              "      <td>0.362159</td>\n",
              "      <td>0.537634</td>\n",
              "      <td>0.027993</td>\n",
              "      <td>0.351621</td>\n",
              "      <td>0.850858</td>\n",
              "      <td>0.022552</td>\n",
              "      <td>0.824087</td>\n",
              "      <td>0.623409</td>\n",
              "      <td>0.272215</td>\n",
              "      <td>0.149312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10239</th>\n",
              "      <td>0.086526</td>\n",
              "      <td>0.026597</td>\n",
              "      <td>0.336040</td>\n",
              "      <td>0.636985</td>\n",
              "      <td>0.273695</td>\n",
              "      <td>0.028798</td>\n",
              "      <td>0.016689</td>\n",
              "      <td>0.482124</td>\n",
              "      <td>0.012084</td>\n",
              "      <td>0.148487</td>\n",
              "      <td>0.272988</td>\n",
              "      <td>0.205471</td>\n",
              "      <td>0.003672</td>\n",
              "      <td>0.512908</td>\n",
              "      <td>0.025286</td>\n",
              "      <td>0.234614</td>\n",
              "      <td>0.044284</td>\n",
              "      <td>0.624747</td>\n",
              "      <td>0.139650</td>\n",
              "      <td>0.144636</td>\n",
              "      <td>0.827394</td>\n",
              "      <td>0.182868</td>\n",
              "      <td>0.029821</td>\n",
              "      <td>0.016948</td>\n",
              "      <td>0.749392</td>\n",
              "      <td>0.364960</td>\n",
              "      <td>0.456795</td>\n",
              "      <td>0.037093</td>\n",
              "      <td>0.344340</td>\n",
              "      <td>0.064000</td>\n",
              "      <td>0.014121</td>\n",
              "      <td>0.039703</td>\n",
              "      <td>0.298929</td>\n",
              "      <td>0.132772</td>\n",
              "      <td>0.069139</td>\n",
              "      <td>0.133821</td>\n",
              "      <td>0.130988</td>\n",
              "      <td>0.027289</td>\n",
              "      <td>0.539169</td>\n",
              "      <td>0.027620</td>\n",
              "      <td>...</td>\n",
              "      <td>0.537755</td>\n",
              "      <td>0.213827</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.025516</td>\n",
              "      <td>0.010442</td>\n",
              "      <td>0.102620</td>\n",
              "      <td>0.021147</td>\n",
              "      <td>0.491935</td>\n",
              "      <td>0.225585</td>\n",
              "      <td>0.190689</td>\n",
              "      <td>0.482721</td>\n",
              "      <td>0.022004</td>\n",
              "      <td>0.567045</td>\n",
              "      <td>0.529292</td>\n",
              "      <td>0.170313</td>\n",
              "      <td>0.581814</td>\n",
              "      <td>0.176537</td>\n",
              "      <td>0.836583</td>\n",
              "      <td>0.389777</td>\n",
              "      <td>0.624050</td>\n",
              "      <td>0.622376</td>\n",
              "      <td>0.484220</td>\n",
              "      <td>0.010010</td>\n",
              "      <td>0.683694</td>\n",
              "      <td>0.011774</td>\n",
              "      <td>0.816721</td>\n",
              "      <td>0.134647</td>\n",
              "      <td>0.518729</td>\n",
              "      <td>0.377881</td>\n",
              "      <td>0.362421</td>\n",
              "      <td>0.367100</td>\n",
              "      <td>0.724592</td>\n",
              "      <td>0.036991</td>\n",
              "      <td>0.472295</td>\n",
              "      <td>0.561289</td>\n",
              "      <td>0.210783</td>\n",
              "      <td>0.876209</td>\n",
              "      <td>0.392504</td>\n",
              "      <td>0.351284</td>\n",
              "      <td>0.165367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10240</th>\n",
              "      <td>0.070902</td>\n",
              "      <td>0.022783</td>\n",
              "      <td>0.066989</td>\n",
              "      <td>0.481585</td>\n",
              "      <td>0.018876</td>\n",
              "      <td>0.028798</td>\n",
              "      <td>0.018316</td>\n",
              "      <td>0.541108</td>\n",
              "      <td>0.009109</td>\n",
              "      <td>0.213401</td>\n",
              "      <td>0.171347</td>\n",
              "      <td>0.149437</td>\n",
              "      <td>0.011016</td>\n",
              "      <td>0.593383</td>\n",
              "      <td>0.036021</td>\n",
              "      <td>0.327805</td>\n",
              "      <td>0.047612</td>\n",
              "      <td>0.029332</td>\n",
              "      <td>0.157490</td>\n",
              "      <td>0.560601</td>\n",
              "      <td>0.844585</td>\n",
              "      <td>0.236427</td>\n",
              "      <td>0.044530</td>\n",
              "      <td>0.032548</td>\n",
              "      <td>0.229631</td>\n",
              "      <td>0.063046</td>\n",
              "      <td>0.319523</td>\n",
              "      <td>0.048956</td>\n",
              "      <td>0.072589</td>\n",
              "      <td>0.064340</td>\n",
              "      <td>0.013974</td>\n",
              "      <td>0.031294</td>\n",
              "      <td>0.267649</td>\n",
              "      <td>0.133130</td>\n",
              "      <td>0.072604</td>\n",
              "      <td>0.123186</td>\n",
              "      <td>0.145947</td>\n",
              "      <td>0.382690</td>\n",
              "      <td>0.713030</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>...</td>\n",
              "      <td>0.604648</td>\n",
              "      <td>0.531024</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.024452</td>\n",
              "      <td>0.010442</td>\n",
              "      <td>0.627001</td>\n",
              "      <td>0.022159</td>\n",
              "      <td>0.746976</td>\n",
              "      <td>0.207512</td>\n",
              "      <td>0.118622</td>\n",
              "      <td>0.712418</td>\n",
              "      <td>0.279359</td>\n",
              "      <td>0.724634</td>\n",
              "      <td>0.275539</td>\n",
              "      <td>0.155233</td>\n",
              "      <td>0.742476</td>\n",
              "      <td>0.058341</td>\n",
              "      <td>0.816819</td>\n",
              "      <td>0.761611</td>\n",
              "      <td>0.165681</td>\n",
              "      <td>0.346462</td>\n",
              "      <td>0.548289</td>\n",
              "      <td>0.010963</td>\n",
              "      <td>0.660609</td>\n",
              "      <td>0.017251</td>\n",
              "      <td>0.829813</td>\n",
              "      <td>0.092679</td>\n",
              "      <td>0.231854</td>\n",
              "      <td>0.568117</td>\n",
              "      <td>0.454304</td>\n",
              "      <td>0.720286</td>\n",
              "      <td>0.449731</td>\n",
              "      <td>0.037491</td>\n",
              "      <td>0.579912</td>\n",
              "      <td>0.436846</td>\n",
              "      <td>0.054925</td>\n",
              "      <td>0.443633</td>\n",
              "      <td>0.329562</td>\n",
              "      <td>0.412561</td>\n",
              "      <td>0.145137</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10241 rows × 151 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c7624fe-05f4-400f-b632-3caa37fcd577')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c7624fe-05f4-400f-b632-3caa37fcd577 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c7624fe-05f4-400f-b632-3caa37fcd577');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            0         1         2    ...       148       149       150\n",
              "0      0.101724  0.029776  0.062929  ...  0.413013  0.448460  0.201133\n",
              "1      0.104126  0.016637  0.054497  ...  0.395686  0.272417  0.195889\n",
              "2      0.068141  0.025432  0.055903  ...  0.358557  0.327330  0.233047\n",
              "3      0.070115  0.033591  0.373985  ...  0.748939  0.270657  0.359184\n",
              "4      0.101594  0.023630  0.375390  ...  0.454031  0.248016  0.251950\n",
              "...         ...       ...       ...  ...       ...       ...       ...\n",
              "10236  0.087833  0.020451  0.056059  ...  0.598303  0.372009  0.229196\n",
              "10237  0.082773  0.020451  0.056059  ...  0.611740  0.437244  0.149150\n",
              "10238  0.088552  0.020451  0.056059  ...  0.623409  0.272215  0.149312\n",
              "10239  0.086526  0.026597  0.336040  ...  0.392504  0.351284  0.165367\n",
              "10240  0.070902  0.022783  0.066989  ...  0.329562  0.412561  0.145137\n",
              "\n",
              "[10241 rows x 151 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Normalization of dataset\n",
        "norm = MinMaxScaler().fit_transform(df.drop('target',axis=1).values)\n",
        "pd.DataFrame(norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50G5MIJQU2yA",
        "outputId": "58cf06b8-9198-4d80-df1f-f969d956abb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.38243998, -0.23861713, -0.31153198, ..., -0.42422882,\n",
              "         1.39091342, -1.10978328],\n",
              "       [ 0.45707678, -0.44603864, -0.39621069, ..., -0.50533914,\n",
              "        -0.67756249, -1.16217142],\n",
              "       [-0.66119443, -0.30720005, -0.38209757, ..., -0.67914698,\n",
              "        -0.03234748, -0.79092709],\n",
              "       ...,\n",
              "       [-0.02690288, -0.3858195 , -0.38052945, ...,  0.56068229,\n",
              "        -0.67993495, -1.62752036],\n",
              "       [-0.08986789, -0.28879975,  2.43111736, ..., -0.52023696,\n",
              "         0.24910389, -1.46712211],\n",
              "       [-0.57539841, -0.3490189 , -0.27076075, ..., -0.81488263,\n",
              "         0.96909826, -1.66923684]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Data Standardization\n",
        "sc = StandardScaler()\n",
        "std = sc.fit_transform(norm)\n",
        "std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "hCd4riDUU2yA",
        "outputId": "2aaf99e6-050b-4a48-bebd-a4b503b21df1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b9426004-6a43-4fab-942c-62464ab2f371\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.382440</td>\n",
              "      <td>-0.238617</td>\n",
              "      <td>-0.311532</td>\n",
              "      <td>1.088893</td>\n",
              "      <td>-0.393482</td>\n",
              "      <td>-0.213872</td>\n",
              "      <td>-0.387492</td>\n",
              "      <td>1.750402</td>\n",
              "      <td>-0.062926</td>\n",
              "      <td>0.933787</td>\n",
              "      <td>-0.707488</td>\n",
              "      <td>-0.500189</td>\n",
              "      <td>-0.176138</td>\n",
              "      <td>0.032384</td>\n",
              "      <td>-0.324696</td>\n",
              "      <td>1.113488</td>\n",
              "      <td>-0.161432</td>\n",
              "      <td>-0.164954</td>\n",
              "      <td>0.766833</td>\n",
              "      <td>-1.355660</td>\n",
              "      <td>-0.352807</td>\n",
              "      <td>0.591209</td>\n",
              "      <td>0.596269</td>\n",
              "      <td>-0.235299</td>\n",
              "      <td>-1.678992</td>\n",
              "      <td>0.360199</td>\n",
              "      <td>0.374389</td>\n",
              "      <td>-0.236662</td>\n",
              "      <td>-0.077710</td>\n",
              "      <td>0.050640</td>\n",
              "      <td>-0.153725</td>\n",
              "      <td>-0.164894</td>\n",
              "      <td>-0.173249</td>\n",
              "      <td>0.026714</td>\n",
              "      <td>-0.083737</td>\n",
              "      <td>-0.229578</td>\n",
              "      <td>-0.820150</td>\n",
              "      <td>-0.091822</td>\n",
              "      <td>-0.690484</td>\n",
              "      <td>-0.451554</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.672354</td>\n",
              "      <td>-0.337156</td>\n",
              "      <td>0.035959</td>\n",
              "      <td>-0.152925</td>\n",
              "      <td>1.563026</td>\n",
              "      <td>-0.091852</td>\n",
              "      <td>1.161925</td>\n",
              "      <td>-0.600015</td>\n",
              "      <td>-1.182788</td>\n",
              "      <td>1.756109</td>\n",
              "      <td>3.172041</td>\n",
              "      <td>0.268873</td>\n",
              "      <td>0.364959</td>\n",
              "      <td>1.004888</td>\n",
              "      <td>-0.759294</td>\n",
              "      <td>-1.558380</td>\n",
              "      <td>-2.408748</td>\n",
              "      <td>1.623176</td>\n",
              "      <td>0.472226</td>\n",
              "      <td>-2.190699</td>\n",
              "      <td>0.055044</td>\n",
              "      <td>-0.056567</td>\n",
              "      <td>-0.467249</td>\n",
              "      <td>-0.028683</td>\n",
              "      <td>-1.060048</td>\n",
              "      <td>-0.403640</td>\n",
              "      <td>0.263341</td>\n",
              "      <td>-0.104191</td>\n",
              "      <td>0.364673</td>\n",
              "      <td>-0.667016</td>\n",
              "      <td>0.720459</td>\n",
              "      <td>0.102919</td>\n",
              "      <td>1.383126</td>\n",
              "      <td>1.088839</td>\n",
              "      <td>0.816223</td>\n",
              "      <td>0.771606</td>\n",
              "      <td>-0.424229</td>\n",
              "      <td>1.390913</td>\n",
              "      <td>-1.109783</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.457077</td>\n",
              "      <td>-0.446039</td>\n",
              "      <td>-0.396211</td>\n",
              "      <td>-0.796533</td>\n",
              "      <td>-0.350809</td>\n",
              "      <td>-0.163167</td>\n",
              "      <td>1.961015</td>\n",
              "      <td>-0.214407</td>\n",
              "      <td>-0.099545</td>\n",
              "      <td>1.132444</td>\n",
              "      <td>-0.691684</td>\n",
              "      <td>1.673859</td>\n",
              "      <td>-0.176138</td>\n",
              "      <td>0.181867</td>\n",
              "      <td>-0.364835</td>\n",
              "      <td>-1.402629</td>\n",
              "      <td>-0.394222</td>\n",
              "      <td>-0.151682</td>\n",
              "      <td>1.970126</td>\n",
              "      <td>0.862492</td>\n",
              "      <td>0.908734</td>\n",
              "      <td>-0.566766</td>\n",
              "      <td>0.121836</td>\n",
              "      <td>-0.217240</td>\n",
              "      <td>0.210901</td>\n",
              "      <td>-1.894167</td>\n",
              "      <td>0.364322</td>\n",
              "      <td>-0.141020</td>\n",
              "      <td>-0.002311</td>\n",
              "      <td>-0.950094</td>\n",
              "      <td>-0.138727</td>\n",
              "      <td>0.177474</td>\n",
              "      <td>-1.945066</td>\n",
              "      <td>-1.644559</td>\n",
              "      <td>-0.086402</td>\n",
              "      <td>2.322001</td>\n",
              "      <td>-0.880916</td>\n",
              "      <td>-0.104941</td>\n",
              "      <td>-0.850916</td>\n",
              "      <td>-0.357521</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056526</td>\n",
              "      <td>-1.195105</td>\n",
              "      <td>-0.145491</td>\n",
              "      <td>-0.064670</td>\n",
              "      <td>-0.999847</td>\n",
              "      <td>-0.117632</td>\n",
              "      <td>-1.173620</td>\n",
              "      <td>-0.716639</td>\n",
              "      <td>0.099329</td>\n",
              "      <td>-0.023445</td>\n",
              "      <td>-0.262908</td>\n",
              "      <td>-1.180387</td>\n",
              "      <td>-0.126822</td>\n",
              "      <td>1.521568</td>\n",
              "      <td>1.434933</td>\n",
              "      <td>1.094874</td>\n",
              "      <td>0.127318</td>\n",
              "      <td>0.355954</td>\n",
              "      <td>0.767280</td>\n",
              "      <td>1.121999</td>\n",
              "      <td>0.200562</td>\n",
              "      <td>-0.083064</td>\n",
              "      <td>-0.510185</td>\n",
              "      <td>-0.188725</td>\n",
              "      <td>-0.636996</td>\n",
              "      <td>0.075542</td>\n",
              "      <td>0.644502</td>\n",
              "      <td>-0.708013</td>\n",
              "      <td>0.401436</td>\n",
              "      <td>-0.851383</td>\n",
              "      <td>-1.073923</td>\n",
              "      <td>-0.010652</td>\n",
              "      <td>-0.722755</td>\n",
              "      <td>1.167961</td>\n",
              "      <td>1.632029</td>\n",
              "      <td>0.333067</td>\n",
              "      <td>-0.505339</td>\n",
              "      <td>-0.677562</td>\n",
              "      <td>-1.162171</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.661194</td>\n",
              "      <td>-0.307200</td>\n",
              "      <td>-0.382098</td>\n",
              "      <td>1.169637</td>\n",
              "      <td>-0.391401</td>\n",
              "      <td>-0.240560</td>\n",
              "      <td>-0.413075</td>\n",
              "      <td>0.842184</td>\n",
              "      <td>-0.172782</td>\n",
              "      <td>-0.912195</td>\n",
              "      <td>0.396942</td>\n",
              "      <td>-0.316589</td>\n",
              "      <td>-0.107198</td>\n",
              "      <td>0.574976</td>\n",
              "      <td>-0.396947</td>\n",
              "      <td>0.257675</td>\n",
              "      <td>-0.201177</td>\n",
              "      <td>-0.149786</td>\n",
              "      <td>-0.529028</td>\n",
              "      <td>-1.516945</td>\n",
              "      <td>0.737483</td>\n",
              "      <td>0.487657</td>\n",
              "      <td>0.692165</td>\n",
              "      <td>-0.219905</td>\n",
              "      <td>-0.844055</td>\n",
              "      <td>-0.257804</td>\n",
              "      <td>-1.682512</td>\n",
              "      <td>-0.317867</td>\n",
              "      <td>-0.167246</td>\n",
              "      <td>-0.083988</td>\n",
              "      <td>-0.203217</td>\n",
              "      <td>0.179449</td>\n",
              "      <td>0.908492</td>\n",
              "      <td>-0.063625</td>\n",
              "      <td>-0.158373</td>\n",
              "      <td>-0.669189</td>\n",
              "      <td>0.346355</td>\n",
              "      <td>-0.113686</td>\n",
              "      <td>-0.207183</td>\n",
              "      <td>-0.501391</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.797993</td>\n",
              "      <td>-0.980618</td>\n",
              "      <td>-0.236215</td>\n",
              "      <td>-0.097450</td>\n",
              "      <td>-0.335293</td>\n",
              "      <td>-0.256844</td>\n",
              "      <td>1.025385</td>\n",
              "      <td>-0.343521</td>\n",
              "      <td>0.296577</td>\n",
              "      <td>-1.218425</td>\n",
              "      <td>-0.298867</td>\n",
              "      <td>-0.482746</td>\n",
              "      <td>0.061627</td>\n",
              "      <td>-0.753136</td>\n",
              "      <td>-1.525047</td>\n",
              "      <td>0.335735</td>\n",
              "      <td>0.240979</td>\n",
              "      <td>-1.038921</td>\n",
              "      <td>-1.030090</td>\n",
              "      <td>0.672909</td>\n",
              "      <td>-0.088481</td>\n",
              "      <td>-0.056567</td>\n",
              "      <td>0.047985</td>\n",
              "      <td>-0.135378</td>\n",
              "      <td>1.639580</td>\n",
              "      <td>-0.394109</td>\n",
              "      <td>-0.327734</td>\n",
              "      <td>-0.066055</td>\n",
              "      <td>-0.130079</td>\n",
              "      <td>1.113496</td>\n",
              "      <td>-0.153509</td>\n",
              "      <td>-0.086367</td>\n",
              "      <td>0.532458</td>\n",
              "      <td>0.230550</td>\n",
              "      <td>0.463881</td>\n",
              "      <td>0.569843</td>\n",
              "      <td>-0.679147</td>\n",
              "      <td>-0.032347</td>\n",
              "      <td>-0.790927</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.599845</td>\n",
              "      <td>-0.178398</td>\n",
              "      <td>2.812172</td>\n",
              "      <td>2.289568</td>\n",
              "      <td>-0.325830</td>\n",
              "      <td>-0.328627</td>\n",
              "      <td>-0.364468</td>\n",
              "      <td>-0.703315</td>\n",
              "      <td>-0.035462</td>\n",
              "      <td>-0.320458</td>\n",
              "      <td>-0.573618</td>\n",
              "      <td>-0.551280</td>\n",
              "      <td>-0.107198</td>\n",
              "      <td>-0.014017</td>\n",
              "      <td>3.552762</td>\n",
              "      <td>0.694156</td>\n",
              "      <td>-0.158594</td>\n",
              "      <td>-0.138410</td>\n",
              "      <td>-0.255921</td>\n",
              "      <td>-0.030124</td>\n",
              "      <td>0.899241</td>\n",
              "      <td>-0.615434</td>\n",
              "      <td>-0.348392</td>\n",
              "      <td>0.267980</td>\n",
              "      <td>0.213910</td>\n",
              "      <td>-3.130172</td>\n",
              "      <td>-1.246637</td>\n",
              "      <td>-0.427945</td>\n",
              "      <td>-0.134259</td>\n",
              "      <td>0.037177</td>\n",
              "      <td>-0.212216</td>\n",
              "      <td>-0.016044</td>\n",
              "      <td>0.296003</td>\n",
              "      <td>0.017680</td>\n",
              "      <td>-0.278325</td>\n",
              "      <td>-0.988228</td>\n",
              "      <td>1.160195</td>\n",
              "      <td>-0.083077</td>\n",
              "      <td>0.838634</td>\n",
              "      <td>-0.269131</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.770757</td>\n",
              "      <td>0.306306</td>\n",
              "      <td>0.104002</td>\n",
              "      <td>-0.029368</td>\n",
              "      <td>0.582560</td>\n",
              "      <td>-0.170910</td>\n",
              "      <td>-1.640729</td>\n",
              "      <td>1.510539</td>\n",
              "      <td>-0.112009</td>\n",
              "      <td>0.484215</td>\n",
              "      <td>-0.353662</td>\n",
              "      <td>-1.047988</td>\n",
              "      <td>0.270868</td>\n",
              "      <td>0.126412</td>\n",
              "      <td>1.406992</td>\n",
              "      <td>-0.147128</td>\n",
              "      <td>0.034968</td>\n",
              "      <td>1.125963</td>\n",
              "      <td>1.271611</td>\n",
              "      <td>-0.700778</td>\n",
              "      <td>0.132787</td>\n",
              "      <td>-0.072465</td>\n",
              "      <td>-0.059355</td>\n",
              "      <td>0.067342</td>\n",
              "      <td>1.364448</td>\n",
              "      <td>-0.435595</td>\n",
              "      <td>-0.546330</td>\n",
              "      <td>-0.116904</td>\n",
              "      <td>-0.713978</td>\n",
              "      <td>0.259910</td>\n",
              "      <td>-0.525080</td>\n",
              "      <td>0.083990</td>\n",
              "      <td>-0.900036</td>\n",
              "      <td>-1.982010</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.198361</td>\n",
              "      <td>1.148318</td>\n",
              "      <td>-0.698244</td>\n",
              "      <td>0.469299</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.378408</td>\n",
              "      <td>-0.335637</td>\n",
              "      <td>2.826285</td>\n",
              "      <td>-0.802825</td>\n",
              "      <td>-0.356014</td>\n",
              "      <td>-0.137814</td>\n",
              "      <td>-0.296673</td>\n",
              "      <td>-0.674646</td>\n",
              "      <td>-0.136163</td>\n",
              "      <td>-0.497866</td>\n",
              "      <td>0.277017</td>\n",
              "      <td>1.754949</td>\n",
              "      <td>-0.176138</td>\n",
              "      <td>-0.112874</td>\n",
              "      <td>-0.340751</td>\n",
              "      <td>-1.317874</td>\n",
              "      <td>-0.776053</td>\n",
              "      <td>-0.138410</td>\n",
              "      <td>0.645138</td>\n",
              "      <td>1.430579</td>\n",
              "      <td>0.138986</td>\n",
              "      <td>-0.313741</td>\n",
              "      <td>-0.366898</td>\n",
              "      <td>0.265612</td>\n",
              "      <td>0.282457</td>\n",
              "      <td>0.360199</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>-0.133802</td>\n",
              "      <td>-0.007023</td>\n",
              "      <td>-0.968045</td>\n",
              "      <td>-0.150725</td>\n",
              "      <td>-0.250070</td>\n",
              "      <td>-0.443311</td>\n",
              "      <td>1.381800</td>\n",
              "      <td>-0.081071</td>\n",
              "      <td>2.054729</td>\n",
              "      <td>1.288239</td>\n",
              "      <td>0.030613</td>\n",
              "      <td>-0.613276</td>\n",
              "      <td>-0.344357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.789176</td>\n",
              "      <td>-1.624080</td>\n",
              "      <td>-0.039645</td>\n",
              "      <td>-0.054584</td>\n",
              "      <td>0.807398</td>\n",
              "      <td>-0.076383</td>\n",
              "      <td>-1.146671</td>\n",
              "      <td>-0.839410</td>\n",
              "      <td>-0.591042</td>\n",
              "      <td>-1.017560</td>\n",
              "      <td>-0.172154</td>\n",
              "      <td>-0.028516</td>\n",
              "      <td>-0.210932</td>\n",
              "      <td>-1.371790</td>\n",
              "      <td>-0.584664</td>\n",
              "      <td>-0.077436</td>\n",
              "      <td>0.319121</td>\n",
              "      <td>-0.845023</td>\n",
              "      <td>0.145497</td>\n",
              "      <td>-0.343267</td>\n",
              "      <td>0.648081</td>\n",
              "      <td>-0.061867</td>\n",
              "      <td>0.187528</td>\n",
              "      <td>-0.268746</td>\n",
              "      <td>0.649086</td>\n",
              "      <td>-0.590447</td>\n",
              "      <td>-0.319448</td>\n",
              "      <td>2.571692</td>\n",
              "      <td>0.882437</td>\n",
              "      <td>1.272398</td>\n",
              "      <td>-1.964921</td>\n",
              "      <td>-0.023271</td>\n",
              "      <td>-1.170692</td>\n",
              "      <td>-0.257278</td>\n",
              "      <td>-0.718817</td>\n",
              "      <td>0.833322</td>\n",
              "      <td>-0.232213</td>\n",
              "      <td>-0.964268</td>\n",
              "      <td>-0.602071</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10236</th>\n",
              "      <td>-0.049255</td>\n",
              "      <td>-0.385819</td>\n",
              "      <td>-0.380529</td>\n",
              "      <td>-1.062883</td>\n",
              "      <td>-0.424707</td>\n",
              "      <td>-0.103120</td>\n",
              "      <td>-0.400284</td>\n",
              "      <td>1.092339</td>\n",
              "      <td>-0.159050</td>\n",
              "      <td>0.143263</td>\n",
              "      <td>0.317922</td>\n",
              "      <td>0.335343</td>\n",
              "      <td>0.030684</td>\n",
              "      <td>-1.242450</td>\n",
              "      <td>-0.355469</td>\n",
              "      <td>-1.194135</td>\n",
              "      <td>-0.177046</td>\n",
              "      <td>-0.204769</td>\n",
              "      <td>0.113210</td>\n",
              "      <td>-0.598211</td>\n",
              "      <td>0.458497</td>\n",
              "      <td>-0.028597</td>\n",
              "      <td>-0.383722</td>\n",
              "      <td>-0.232042</td>\n",
              "      <td>0.474724</td>\n",
              "      <td>1.177037</td>\n",
              "      <td>-1.116445</td>\n",
              "      <td>-0.417118</td>\n",
              "      <td>-0.155465</td>\n",
              "      <td>0.019227</td>\n",
              "      <td>-0.104232</td>\n",
              "      <td>-0.159810</td>\n",
              "      <td>-2.079351</td>\n",
              "      <td>0.005635</td>\n",
              "      <td>-0.051750</td>\n",
              "      <td>-0.000319</td>\n",
              "      <td>-0.812554</td>\n",
              "      <td>-0.096195</td>\n",
              "      <td>-1.063489</td>\n",
              "      <td>-0.276653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.685384</td>\n",
              "      <td>-1.195105</td>\n",
              "      <td>-0.016964</td>\n",
              "      <td>-0.142838</td>\n",
              "      <td>-1.248877</td>\n",
              "      <td>-0.086696</td>\n",
              "      <td>-1.204161</td>\n",
              "      <td>-0.378489</td>\n",
              "      <td>0.381113</td>\n",
              "      <td>0.814392</td>\n",
              "      <td>-0.187565</td>\n",
              "      <td>-0.370716</td>\n",
              "      <td>-0.566103</td>\n",
              "      <td>1.044681</td>\n",
              "      <td>1.385163</td>\n",
              "      <td>-0.957291</td>\n",
              "      <td>0.276498</td>\n",
              "      <td>0.945091</td>\n",
              "      <td>-0.845014</td>\n",
              "      <td>0.118151</td>\n",
              "      <td>0.857388</td>\n",
              "      <td>-0.183752</td>\n",
              "      <td>0.112389</td>\n",
              "      <td>-0.060692</td>\n",
              "      <td>-1.516949</td>\n",
              "      <td>-0.128403</td>\n",
              "      <td>-0.672200</td>\n",
              "      <td>-0.108429</td>\n",
              "      <td>0.352699</td>\n",
              "      <td>-1.330125</td>\n",
              "      <td>-2.286176</td>\n",
              "      <td>-0.187319</td>\n",
              "      <td>-0.155748</td>\n",
              "      <td>0.451764</td>\n",
              "      <td>-0.525543</td>\n",
              "      <td>0.607228</td>\n",
              "      <td>0.443155</td>\n",
              "      <td>0.492623</td>\n",
              "      <td>-0.829410</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10237</th>\n",
              "      <td>-0.206484</td>\n",
              "      <td>-0.385819</td>\n",
              "      <td>-0.380529</td>\n",
              "      <td>-1.062883</td>\n",
              "      <td>-0.424707</td>\n",
              "      <td>-0.103120</td>\n",
              "      <td>-0.400284</td>\n",
              "      <td>1.724724</td>\n",
              "      <td>-0.159050</td>\n",
              "      <td>0.432499</td>\n",
              "      <td>0.317922</td>\n",
              "      <td>-0.137640</td>\n",
              "      <td>0.306446</td>\n",
              "      <td>-0.063768</td>\n",
              "      <td>-0.355469</td>\n",
              "      <td>-1.006983</td>\n",
              "      <td>-0.177046</td>\n",
              "      <td>-0.204769</td>\n",
              "      <td>-0.851500</td>\n",
              "      <td>-1.336724</td>\n",
              "      <td>0.212662</td>\n",
              "      <td>0.969634</td>\n",
              "      <td>1.710851</td>\n",
              "      <td>-0.231746</td>\n",
              "      <td>0.347326</td>\n",
              "      <td>-0.257804</td>\n",
              "      <td>-1.116445</td>\n",
              "      <td>-0.417118</td>\n",
              "      <td>-0.155465</td>\n",
              "      <td>0.019227</td>\n",
              "      <td>-0.104232</td>\n",
              "      <td>-0.239424</td>\n",
              "      <td>-1.670528</td>\n",
              "      <td>0.005635</td>\n",
              "      <td>-0.051750</td>\n",
              "      <td>-0.114158</td>\n",
              "      <td>0.439675</td>\n",
              "      <td>-0.096195</td>\n",
              "      <td>1.006085</td>\n",
              "      <td>-0.276653</td>\n",
              "      <td>...</td>\n",
              "      <td>2.323160</td>\n",
              "      <td>-1.195105</td>\n",
              "      <td>-0.016964</td>\n",
              "      <td>-0.142838</td>\n",
              "      <td>1.356687</td>\n",
              "      <td>-0.086696</td>\n",
              "      <td>0.646308</td>\n",
              "      <td>-1.012328</td>\n",
              "      <td>-1.225056</td>\n",
              "      <td>1.757858</td>\n",
              "      <td>-0.187565</td>\n",
              "      <td>-0.589684</td>\n",
              "      <td>-0.462226</td>\n",
              "      <td>0.828436</td>\n",
              "      <td>1.351983</td>\n",
              "      <td>-0.817908</td>\n",
              "      <td>0.141526</td>\n",
              "      <td>0.377167</td>\n",
              "      <td>0.376485</td>\n",
              "      <td>-0.068530</td>\n",
              "      <td>1.704584</td>\n",
              "      <td>-0.183752</td>\n",
              "      <td>-0.907344</td>\n",
              "      <td>-0.060692</td>\n",
              "      <td>-1.178503</td>\n",
              "      <td>-0.178817</td>\n",
              "      <td>0.156017</td>\n",
              "      <td>-0.108429</td>\n",
              "      <td>0.450172</td>\n",
              "      <td>0.181477</td>\n",
              "      <td>-2.177801</td>\n",
              "      <td>-0.187319</td>\n",
              "      <td>0.624578</td>\n",
              "      <td>-1.167648</td>\n",
              "      <td>-0.716090</td>\n",
              "      <td>-1.406845</td>\n",
              "      <td>0.506057</td>\n",
              "      <td>1.259128</td>\n",
              "      <td>-1.629137</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10238</th>\n",
              "      <td>-0.026903</td>\n",
              "      <td>-0.385819</td>\n",
              "      <td>-0.380529</td>\n",
              "      <td>-1.062883</td>\n",
              "      <td>-0.424707</td>\n",
              "      <td>-0.103120</td>\n",
              "      <td>-0.400284</td>\n",
              "      <td>0.260880</td>\n",
              "      <td>-0.159050</td>\n",
              "      <td>-0.575050</td>\n",
              "      <td>0.317922</td>\n",
              "      <td>0.914472</td>\n",
              "      <td>-0.314020</td>\n",
              "      <td>2.444807</td>\n",
              "      <td>6.399978</td>\n",
              "      <td>-1.497746</td>\n",
              "      <td>-0.177046</td>\n",
              "      <td>-0.204769</td>\n",
              "      <td>-0.701249</td>\n",
              "      <td>-0.044489</td>\n",
              "      <td>0.992215</td>\n",
              "      <td>-1.045276</td>\n",
              "      <td>2.705142</td>\n",
              "      <td>-0.180234</td>\n",
              "      <td>1.373192</td>\n",
              "      <td>-0.050907</td>\n",
              "      <td>1.056219</td>\n",
              "      <td>-0.417118</td>\n",
              "      <td>-0.155465</td>\n",
              "      <td>0.019227</td>\n",
              "      <td>-0.104232</td>\n",
              "      <td>-0.170768</td>\n",
              "      <td>-0.101630</td>\n",
              "      <td>0.005635</td>\n",
              "      <td>-0.051750</td>\n",
              "      <td>-0.141785</td>\n",
              "      <td>0.439675</td>\n",
              "      <td>-0.096195</td>\n",
              "      <td>-1.063489</td>\n",
              "      <td>4.343161</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.641017</td>\n",
              "      <td>1.164255</td>\n",
              "      <td>-0.016964</td>\n",
              "      <td>-0.142838</td>\n",
              "      <td>0.470141</td>\n",
              "      <td>-0.086696</td>\n",
              "      <td>-1.820386</td>\n",
              "      <td>-1.640018</td>\n",
              "      <td>0.493826</td>\n",
              "      <td>-1.389415</td>\n",
              "      <td>-0.187565</td>\n",
              "      <td>-0.589684</td>\n",
              "      <td>-0.368938</td>\n",
              "      <td>-1.052877</td>\n",
              "      <td>1.179100</td>\n",
              "      <td>3.449451</td>\n",
              "      <td>0.596171</td>\n",
              "      <td>1.169879</td>\n",
              "      <td>-1.633366</td>\n",
              "      <td>2.974715</td>\n",
              "      <td>-0.106422</td>\n",
              "      <td>-0.183752</td>\n",
              "      <td>0.606155</td>\n",
              "      <td>-0.060692</td>\n",
              "      <td>0.006045</td>\n",
              "      <td>-0.415723</td>\n",
              "      <td>-0.852521</td>\n",
              "      <td>-0.108429</td>\n",
              "      <td>0.221738</td>\n",
              "      <td>-1.330125</td>\n",
              "      <td>-0.354777</td>\n",
              "      <td>-0.187319</td>\n",
              "      <td>-0.464829</td>\n",
              "      <td>1.092627</td>\n",
              "      <td>-1.757147</td>\n",
              "      <td>0.722352</td>\n",
              "      <td>0.560682</td>\n",
              "      <td>-0.679935</td>\n",
              "      <td>-1.627520</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10239</th>\n",
              "      <td>-0.089868</td>\n",
              "      <td>-0.288800</td>\n",
              "      <td>2.431117</td>\n",
              "      <td>1.181172</td>\n",
              "      <td>2.222056</td>\n",
              "      <td>-0.088442</td>\n",
              "      <td>-0.437379</td>\n",
              "      <td>0.690231</td>\n",
              "      <td>-0.401648</td>\n",
              "      <td>-0.722003</td>\n",
              "      <td>1.252225</td>\n",
              "      <td>1.006304</td>\n",
              "      <td>-0.176138</td>\n",
              "      <td>-0.456851</td>\n",
              "      <td>-0.388919</td>\n",
              "      <td>-0.492287</td>\n",
              "      <td>-0.133044</td>\n",
              "      <td>6.611277</td>\n",
              "      <td>-0.492037</td>\n",
              "      <td>-1.254450</td>\n",
              "      <td>0.806019</td>\n",
              "      <td>-0.442085</td>\n",
              "      <td>-0.461112</td>\n",
              "      <td>0.431398</td>\n",
              "      <td>1.240110</td>\n",
              "      <td>0.972827</td>\n",
              "      <td>-0.290664</td>\n",
              "      <td>-0.301626</td>\n",
              "      <td>4.780801</td>\n",
              "      <td>0.005764</td>\n",
              "      <td>-0.129728</td>\n",
              "      <td>-0.078375</td>\n",
              "      <td>-1.522814</td>\n",
              "      <td>-0.003399</td>\n",
              "      <td>-0.059746</td>\n",
              "      <td>-0.080485</td>\n",
              "      <td>-0.865725</td>\n",
              "      <td>-0.078704</td>\n",
              "      <td>-0.424769</td>\n",
              "      <td>-0.267250</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.403716</td>\n",
              "      <td>-1.195105</td>\n",
              "      <td>0.005717</td>\n",
              "      <td>-0.117623</td>\n",
              "      <td>-1.573327</td>\n",
              "      <td>-0.059197</td>\n",
              "      <td>-0.449601</td>\n",
              "      <td>-0.579265</td>\n",
              "      <td>1.113751</td>\n",
              "      <td>-0.941924</td>\n",
              "      <td>-0.238935</td>\n",
              "      <td>0.655885</td>\n",
              "      <td>3.162431</td>\n",
              "      <td>0.239926</td>\n",
              "      <td>0.319047</td>\n",
              "      <td>-0.298956</td>\n",
              "      <td>1.036608</td>\n",
              "      <td>-0.441224</td>\n",
              "      <td>0.580069</td>\n",
              "      <td>0.753921</td>\n",
              "      <td>0.385949</td>\n",
              "      <td>-0.130758</td>\n",
              "      <td>0.874506</td>\n",
              "      <td>-0.098035</td>\n",
              "      <td>1.261284</td>\n",
              "      <td>0.392270</td>\n",
              "      <td>-0.125711</td>\n",
              "      <td>4.645873</td>\n",
              "      <td>-0.369644</td>\n",
              "      <td>-1.291418</td>\n",
              "      <td>1.500760</td>\n",
              "      <td>0.039824</td>\n",
              "      <td>0.532608</td>\n",
              "      <td>-0.426516</td>\n",
              "      <td>1.257919</td>\n",
              "      <td>0.879016</td>\n",
              "      <td>-0.520237</td>\n",
              "      <td>0.249104</td>\n",
              "      <td>-1.467122</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10240</th>\n",
              "      <td>-0.575398</td>\n",
              "      <td>-0.349019</td>\n",
              "      <td>-0.270761</td>\n",
              "      <td>0.132548</td>\n",
              "      <td>-0.419502</td>\n",
              "      <td>-0.088442</td>\n",
              "      <td>-0.407959</td>\n",
              "      <td>1.138450</td>\n",
              "      <td>-0.474886</td>\n",
              "      <td>0.240388</td>\n",
              "      <td>0.296540</td>\n",
              "      <td>0.359022</td>\n",
              "      <td>0.237506</td>\n",
              "      <td>1.214802</td>\n",
              "      <td>-0.268501</td>\n",
              "      <td>0.179459</td>\n",
              "      <td>-0.067749</td>\n",
              "      <td>-0.085323</td>\n",
              "      <td>-0.225806</td>\n",
              "      <td>0.229107</td>\n",
              "      <td>0.878266</td>\n",
              "      <td>0.246472</td>\n",
              "      <td>-0.307173</td>\n",
              "      <td>1.048655</td>\n",
              "      <td>-1.402798</td>\n",
              "      <td>-2.925963</td>\n",
              "      <td>-0.862100</td>\n",
              "      <td>-0.173502</td>\n",
              "      <td>-0.105984</td>\n",
              "      <td>0.014739</td>\n",
              "      <td>-0.134228</td>\n",
              "      <td>-0.574552</td>\n",
              "      <td>-1.766765</td>\n",
              "      <td>0.002623</td>\n",
              "      <td>-0.006435</td>\n",
              "      <td>-0.179718</td>\n",
              "      <td>-0.785426</td>\n",
              "      <td>9.545568</td>\n",
              "      <td>0.883755</td>\n",
              "      <td>-0.210830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216793</td>\n",
              "      <td>0.520793</td>\n",
              "      <td>-0.032085</td>\n",
              "      <td>-0.117623</td>\n",
              "      <td>0.477256</td>\n",
              "      <td>-0.042010</td>\n",
              "      <td>0.913998</td>\n",
              "      <td>-0.761021</td>\n",
              "      <td>-0.478328</td>\n",
              "      <td>0.437735</td>\n",
              "      <td>3.446015</td>\n",
              "      <td>1.816923</td>\n",
              "      <td>-0.076469</td>\n",
              "      <td>-0.072680</td>\n",
              "      <td>0.980895</td>\n",
              "      <td>-1.463799</td>\n",
              "      <td>-0.021862</td>\n",
              "      <td>1.039621</td>\n",
              "      <td>-1.351837</td>\n",
              "      <td>-1.491528</td>\n",
              "      <td>1.059718</td>\n",
              "      <td>-0.109561</td>\n",
              "      <td>-0.134494</td>\n",
              "      <td>0.008660</td>\n",
              "      <td>1.312869</td>\n",
              "      <td>-0.183706</td>\n",
              "      <td>-2.020468</td>\n",
              "      <td>7.601421</td>\n",
              "      <td>0.311166</td>\n",
              "      <td>1.475099</td>\n",
              "      <td>-1.227196</td>\n",
              "      <td>0.052443</td>\n",
              "      <td>1.422124</td>\n",
              "      <td>-1.079373</td>\n",
              "      <td>-1.238601</td>\n",
              "      <td>-0.421172</td>\n",
              "      <td>-0.814883</td>\n",
              "      <td>0.969098</td>\n",
              "      <td>-1.669237</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10241 rows × 152 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9426004-6a43-4fab-942c-62464ab2f371')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9426004-6a43-4fab-942c-62464ab2f371 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9426004-6a43-4fab-942c-62464ab2f371');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              0         1         2  ...       149       150  target\n",
              "0      0.382440 -0.238617 -0.311532  ...  1.390913 -1.109783       0\n",
              "1      0.457077 -0.446039 -0.396211  ... -0.677562 -1.162171       0\n",
              "2     -0.661194 -0.307200 -0.382098  ... -0.032347 -0.790927       0\n",
              "3     -0.599845 -0.178398  2.812172  ... -0.698244  0.469299       0\n",
              "4      0.378408 -0.335637  2.826285  ... -0.964268 -0.602071       0\n",
              "...         ...       ...       ...  ...       ...       ...     ...\n",
              "10236 -0.049255 -0.385819 -0.380529  ...  0.492623 -0.829410       3\n",
              "10237 -0.206484 -0.385819 -0.380529  ...  1.259128 -1.629137       3\n",
              "10238 -0.026903 -0.385819 -0.380529  ... -0.679935 -1.627520       1\n",
              "10239 -0.089868 -0.288800  2.431117  ...  0.249104 -1.467122       1\n",
              "10240 -0.575398 -0.349019 -0.270761  ...  0.969098 -1.669237       4\n",
              "\n",
              "[10241 rows x 152 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_nstd = pd.DataFrame(std)\n",
        "X = pd.concat([X_nstd, df_main['target']],axis=1)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential Functions"
      ],
      "metadata": {
        "id": "OY0IfVEdzllO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk3Yuh_zU2x_"
      },
      "outputs": [],
      "source": [
        "#Splitting dataset to form validation set\n",
        "def split(df):\n",
        "  x_train, x_val, y_train, y_val = train_test_split(df.drop(['target'], axis=1), \\\n",
        "                                                  df['target'].astype('int'),\n",
        "                                                    test_size=0.20,shuffle=True,\n",
        "                                                    random_state=0,\n",
        "                                                    stratify=df['target'])\n",
        "  return x_train, x_val, y_train, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYLvDmnhU2x-"
      },
      "outputs": [],
      "source": [
        "#Prediction report\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "def evaluate_model(y_true, y_pred, index_values):\n",
        "  print('Classification report')\n",
        "  print('-'*75)\n",
        "  print(classification_report(y_true,y_pred))\n",
        "  print('Confusion matrix')\n",
        "  print('-'*75)\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_df = pd.DataFrame(cm.T, index=index_values, columns=index_values)\n",
        "  cm_df.index.name = 'Predicted'\n",
        "  cm_df.columns.name = 'True'\n",
        "  print(cm_df)\n",
        "  print('')\n",
        "  print('Matthew Correlation coefficient')\n",
        "  print('-'*75)\n",
        "  print( matthews_corrcoef(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Over sampling and under sampling\n",
        "def sampling_strategy(kind = \"SMOTETOMEK\"):\n",
        "  if(kind == \"SMOTE\"):\n",
        "    oversample = SMOTE(random_state=42)\n",
        "    X, y = oversample.fit_resample(x_train, y_train)\n",
        "  else:\n",
        "    smt = SMOTETomek(random_state=42)\n",
        "    X, y = smt.fit_resample(x_train, y_train)\n",
        "\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "mw_1UGtZjKYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-test split for model input"
      ],
      "metadata": {
        "id": "z9rcfXhh2dAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THHRBDqsU2yB",
        "outputId": "e17a66b9-44b4-418b-f401-5a560ca93371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset\n",
            "0    7999\n",
            "1      81\n",
            "2      47\n",
            "3      42\n",
            "4      23\n",
            "Name: target, dtype: int64\n",
            "Test dataset\n",
            "0    2001\n",
            "1      20\n",
            "2      12\n",
            "3      10\n",
            "4       6\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Class-wise distribution of target field\n",
        "x_train, x_test, y_train, y_test = split(X)\n",
        "train = pd.concat([x_train,y_train],axis=1)\n",
        "test = pd.concat([x_test, y_test],axis=1)\n",
        "print(\"Train dataset\")\n",
        "print(train['target'].value_counts())\n",
        "print(\"Test dataset\")\n",
        "print(test['target'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model design - Baseline"
      ],
      "metadata": {
        "id": "Gve-YpvYz4P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(a) LGBM Classifier**"
      ],
      "metadata": {
        "id": "iHDE7-oH0DP6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-vOVMpFU2yB"
      },
      "outputs": [],
      "source": [
        "model1 = lgb.LGBMClassifier(random_state=0, n_estimators=500,\n",
        "                           learning_rate=0.1, num_leaves=31,\n",
        "                           is_unbalance=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdJDlvNzU2yB",
        "outputId": "5617b343-13c6-42b7-cc43-e68bfc2fb1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1600\n",
            "           1       0.75      0.19      0.30        16\n",
            "           2       1.00      1.00      1.00        10\n",
            "           3       0.62      0.62      0.62         8\n",
            "           4       1.00      0.20      0.33         5\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.87      0.60      0.65      1639\n",
            "weighted avg       0.99      0.99      0.98      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2  3  4\n",
            "Predicted                    \n",
            "0          1599  12   0  2  3\n",
            "1             0   3   0  1  0\n",
            "2             0   0  10  0  0\n",
            "3             1   1   0  5  1\n",
            "4             0   0   0  0  1\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.681416242705196\n"
          ]
        }
      ],
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "model1.fit(x_train, y_train, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model1.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(b) Random Forest Classifier**"
      ],
      "metadata": {
        "id": "38m-4GhL0GP-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdyseCUzU2yC"
      },
      "outputs": [],
      "source": [
        "model2 = RandomForestClassifier(max_depth=2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e562il_wU2yC",
        "outputId": "fd30f0ba-0996-497c-b2cc-f7533a1c14af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1600\n",
            "           1       0.00      0.00      0.00        16\n",
            "           2       1.00      1.00      1.00        10\n",
            "           3       0.00      0.00      0.00         8\n",
            "           4       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.98      1639\n",
            "   macro avg       0.40      0.40      0.40      1639\n",
            "weighted avg       0.96      0.98      0.97      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2  3  4\n",
            "Predicted                    \n",
            "0          1600  16   0  8  5\n",
            "1             0   0   0  0  0\n",
            "2             0   0  10  0  0\n",
            "3             0   0   0  0  0\n",
            "4             0   0   0  0  0\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.5042211262780502\n"
          ]
        }
      ],
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "model2.fit(x_train, y_train)\n",
        "val_pred = model2.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(c) Catboost Classifier**"
      ],
      "metadata": {
        "id": "OkGjUUo-562B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = catboost.CatBoostClassifier(n_estimators=2500,random_state=0, learning_rate=0.03, verbose=100, early_stopping_rounds=50,auto_class_weights=\"Balanced\")"
      ],
      "metadata": {
        "id": "1dhuHiDw55Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "model3.fit(x_train, y_train)\n",
        "val_pred = model3.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Zuweyc8WkM",
        "outputId": "be005530-94b6-4e5c-bc3a-2774e65b416d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 1.5761818\ttotal: 323ms\tremaining: 13m 27s\n",
            "100:\tlearn: 0.4756234\ttotal: 15.7s\tremaining: 6m 13s\n",
            "200:\tlearn: 0.2160599\ttotal: 31s\tremaining: 5m 54s\n",
            "300:\tlearn: 0.1066280\ttotal: 46.7s\tremaining: 5m 40s\n",
            "400:\tlearn: 0.0513933\ttotal: 1m 2s\tremaining: 5m 26s\n",
            "500:\tlearn: 0.0297073\ttotal: 1m 17s\tremaining: 5m 9s\n",
            "600:\tlearn: 0.0203882\ttotal: 1m 32s\tremaining: 4m 53s\n",
            "700:\tlearn: 0.0157802\ttotal: 1m 47s\tremaining: 4m 36s\n",
            "800:\tlearn: 0.0133208\ttotal: 2m 2s\tremaining: 4m 20s\n",
            "900:\tlearn: 0.0116024\ttotal: 2m 17s\tremaining: 4m 3s\n",
            "1000:\tlearn: 0.0103457\ttotal: 2m 31s\tremaining: 3m 46s\n",
            "1100:\tlearn: 0.0092859\ttotal: 2m 46s\tremaining: 3m 31s\n",
            "1200:\tlearn: 0.0085228\ttotal: 3m 1s\tremaining: 3m 16s\n",
            "1300:\tlearn: 0.0078637\ttotal: 3m 16s\tremaining: 3m 1s\n",
            "1400:\tlearn: 0.0072819\ttotal: 3m 31s\tremaining: 2m 45s\n",
            "1500:\tlearn: 0.0069055\ttotal: 3m 45s\tremaining: 2m 30s\n",
            "1600:\tlearn: 0.0065851\ttotal: 4m\tremaining: 2m 15s\n",
            "1700:\tlearn: 0.0062234\ttotal: 4m 15s\tremaining: 2m\n",
            "1800:\tlearn: 0.0059204\ttotal: 4m 30s\tremaining: 1m 44s\n",
            "1900:\tlearn: 0.0055704\ttotal: 4m 44s\tremaining: 1m 29s\n",
            "2000:\tlearn: 0.0053249\ttotal: 5m\tremaining: 1m 14s\n",
            "2100:\tlearn: 0.0051132\ttotal: 5m 15s\tremaining: 59.9s\n",
            "2200:\tlearn: 0.0048738\ttotal: 5m 30s\tremaining: 44.9s\n",
            "2300:\tlearn: 0.0046498\ttotal: 5m 45s\tremaining: 29.9s\n",
            "2400:\tlearn: 0.0044146\ttotal: 5m 59s\tremaining: 14.8s\n",
            "2499:\tlearn: 0.0042511\ttotal: 6m 14s\tremaining: 0us\n",
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1600\n",
            "           1       0.70      0.44      0.54        16\n",
            "           2       0.91      1.00      0.95        10\n",
            "           3       0.83      0.62      0.71         8\n",
            "           4       1.00      0.20      0.33         5\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.89      0.65      0.71      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0  1   2  3  4\n",
            "Predicted                   \n",
            "0          1598  9   0  3  1\n",
            "1             1  7   0  0  2\n",
            "2             0  0  10  0  1\n",
            "3             1  0   0  5  0\n",
            "4             0  0   0  0  1\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.7382090154518846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMOTE AND SMOTETOMEK sampling strategy on classifiers"
      ],
      "metadata": {
        "id": "rFFlpcHT-_YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(a) On LGBM classifier**"
      ],
      "metadata": {
        "id": "RkFeLn2s2tmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  (i) SMOTE sampling strategy"
      ],
      "metadata": {
        "id": "N86joPBb_N3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "X,y = sampling_strategy(\"SMOTE\")\n",
        "model1.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model1.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okDsJIUNJ0Y8",
        "outputId": "e5d11e21-5ff0-4ae7-e2e7-2cd4be69e3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1600\n",
            "           1       0.86      0.38      0.52        16\n",
            "           2       1.00      0.90      0.95        10\n",
            "           3       0.80      0.50      0.62         8\n",
            "           4       0.80      0.80      0.80         5\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.89      0.71      0.78      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0  1  2  3  4\n",
            "Predicted                  \n",
            "0          1599  9  1  3  1\n",
            "1             0  6  0  1  0\n",
            "2             0  0  9  0  0\n",
            "3             0  1  0  4  0\n",
            "4             1  0  0  0  4\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.7509652128201829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) SMOTETOMEK sampling strategy"
      ],
      "metadata": {
        "id": "a77SBRF828Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "X,y = sampling_strategy(\"SMOTETOMEK\")\n",
        "model1.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model1.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oH9731OLfjx",
        "outputId": "58a264ea-d092-453a-fc89-b2692ebdf9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1600\n",
            "           1       0.86      0.38      0.52        16\n",
            "           2       1.00      0.90      0.95        10\n",
            "           3       0.80      0.50      0.62         8\n",
            "           4       0.80      0.80      0.80         5\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.89      0.71      0.78      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0  1  2  3  4\n",
            "Predicted                  \n",
            "0          1599  9  1  3  1\n",
            "1             0  6  0  1  0\n",
            "2             0  0  9  0  0\n",
            "3             0  1  0  4  0\n",
            "4             1  0  0  0  4\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.7509652128201829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(b) Random Forest Classifier**"
      ],
      "metadata": {
        "id": "rZJaZA5Y_oxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) SMOTE sampling strategy"
      ],
      "metadata": {
        "id": "ykwk80g1_tr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "X,y = sampling_strategy(\"SMOTE\")\n",
        "model2.fit(X, y)\n",
        "val_pred = model2.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1d_oX-x_oN3",
        "outputId": "4ef7b2d3-a5fb-48b7-8555-67d814531b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.64      0.78      1600\n",
            "           1       0.08      0.38      0.13        16\n",
            "           2       0.83      1.00      0.91        10\n",
            "           3       0.01      0.62      0.02         8\n",
            "           4       0.10      0.80      0.17         5\n",
            "\n",
            "    accuracy                           0.64      1639\n",
            "   macro avg       0.40      0.69      0.40      1639\n",
            "weighted avg       0.97      0.64      0.76      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0  1   2  3  4\n",
            "Predicted                   \n",
            "0          1019  6   0  3  1\n",
            "1            73  6   0  0  0\n",
            "2             2  0  10  0  0\n",
            "3           469  4   0  5  0\n",
            "4            37  0   0  0  4\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.14172698876184656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) SMOTETOMEK sampling strategy"
      ],
      "metadata": {
        "id": "F3_flr_wAHGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "X,y = sampling_strategy(\"SMOTETOMEK\")\n",
        "model2.fit(X, y)\n",
        "val_pred = model2.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ZPOAGkAGi3",
        "outputId": "9f3206df-b1bb-409e-8d60-b32e57e55146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.64      0.78      1600\n",
            "           1       0.08      0.38      0.13        16\n",
            "           2       0.83      1.00      0.91        10\n",
            "           3       0.01      0.62      0.02         8\n",
            "           4       0.10      0.80      0.17         5\n",
            "\n",
            "    accuracy                           0.64      1639\n",
            "   macro avg       0.40      0.69      0.40      1639\n",
            "weighted avg       0.97      0.64      0.76      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0  1   2  3  4\n",
            "Predicted                   \n",
            "0          1019  6   0  3  1\n",
            "1            73  6   0  0  0\n",
            "2             2  0  10  0  0\n",
            "3           469  4   0  5  0\n",
            "4            37  0   0  0  4\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.14172698876184656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(c) Catboost classifier**"
      ],
      "metadata": {
        "id": "H2G_tpsIASqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) SMOTE sampling strategy"
      ],
      "metadata": {
        "id": "A18UvS-3AUnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "X,y = sampling_strategy(\"SMOTE\")\n",
        "model3.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model3.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87_e5kZ6AW9X",
        "outputId": "a859eea9-47e2-4c78-8d16-627a8b8d9797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1600\n",
            "           1       0.73      0.50      0.59        16\n",
            "           2       1.00      1.00      1.00        10\n",
            "           3       0.50      0.50      0.50         8\n",
            "           4       0.80      0.80      0.80         5\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.80      0.76      0.78      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0  1   2  3  4\n",
            "Predicted                   \n",
            "0          1595  6   0  3  1\n",
            "1             2  8   0  1  0\n",
            "2             0  0  10  0  0\n",
            "3             2  2   0  4  0\n",
            "4             1  0   0  0  4\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.75175669797641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) SMOTETOMEK sampling strategy"
      ],
      "metadata": {
        "id": "6uTuKvfBAcfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_values = list(train.sort_values(by=\"target\")['target'].unique())\n",
        "x_train, x_val, y_train, y_val = split(train)\n",
        "\n",
        "X,y = sampling_strategy(\"SMOTETOMEK\")\n",
        "model3.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model3.predict(x_val)\n",
        "\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbu1HJMqAdhO",
        "outputId": "17d32df3-493c-42d4-af62-cd513f3aa94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1600\n",
            "           1       0.73      0.50      0.59        16\n",
            "           2       1.00      1.00      1.00        10\n",
            "           3       0.50      0.50      0.50         8\n",
            "           4       0.80      0.80      0.80         5\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.80      0.76      0.78      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0  1   2  3  4\n",
            "Predicted                   \n",
            "0          1595  6   0  3  1\n",
            "1             2  8   0  1  0\n",
            "2             0  0  10  0  0\n",
            "3             2  2   0  4  0\n",
            "4             1  0   0  0  4\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.75175669797641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One vs Rest dataset split on Classifiers"
      ],
      "metadata": {
        "id": "WxGGJQqD3mH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(a) LGBM Classifier-  SMOTE sampling strategy**"
      ],
      "metadata": {
        "id": "mtRFj4vYC8Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onevsrest = train.copy()\n",
        "model1 = lgb.LGBMClassifier(class_weight=\"balanced\")\n",
        "onevsrest['target'] = np.where(train['target'] == 0, 0, 999)\n",
        "x_train, x_val, y_train, y_val = split(onevsrest)\n",
        "index_values = list(onevsrest['target'].unique())\n",
        "\n",
        "X, y = sampling_strategy(\"SMOTE\")\n",
        "\n",
        "model1.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model1.predict(x_val)\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI0ia7E0dmM5",
        "outputId": "e9c5eb5a-c6b9-4803-ea9e-8fe11cb84641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1600\n",
            "         999       0.94      0.74      0.83        39\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.96      0.87      0.91      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True        0    999\n",
            "Predicted           \n",
            "0          1598   10\n",
            "999           2   29\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.8305602423741267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = lgb.LGBMClassifier(random_state=0, n_estimators=500,\n",
        "                           learning_rate=0.1, num_leaves=31,\n",
        "                           is_unbalance=True)\n",
        "df.dropna(axis=1,inplace=True)\n",
        "others = train[train['target'] != 0]\n",
        "index_values = list(others['target'].unique())\n",
        "\n",
        "x_train, x_val, y_train, y_val = split(others)\n",
        "\n",
        "X, y = sampling_strategy(\"SMOTE\")\n",
        "\n",
        "model_1.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model_1.predict(x_val)\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvBisg4XdoFy",
        "outputId": "1b4bbf69-2ca1-40fb-a87f-1f53035d1993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.74      0.88      0.80        16\n",
            "           2       1.00      1.00      1.00        10\n",
            "           3       0.71      0.62      0.67         8\n",
            "           4       1.00      0.60      0.75         5\n",
            "\n",
            "    accuracy                           0.82        39\n",
            "   macro avg       0.86      0.78      0.80        39\n",
            "weighted avg       0.83      0.82      0.82        39\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True        1   2  3  4\n",
            "Predicted              \n",
            "1          14   0  3  2\n",
            "2           0  10  0  0\n",
            "3           2   0  5  0\n",
            "4           0   0  0  3\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.7444567936469952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "final_train = train.copy()\n",
        "final_train['prediction'] = model1.predict(final_train[x_train.columns])\n",
        "print(final_train['prediction'].value_counts())\n",
        "print('')\n",
        "\n",
        "one_class = final_train[final_train['prediction'] == 0]\n",
        "other_class = final_train[final_train['prediction'] != 0]\n",
        "other_class['prediction'] = model_1.predict(other_class[x_train.columns])\n",
        "final_train_df = pd.concat([one_class, other_class],axis=0)\n",
        "evaluate_model(final_train_df['target'], final_train_df['prediction'], [0,1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGsKxdc7sRU6",
        "outputId": "7fea3747-3809-42e6-8fff-78ce115482d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      8007\n",
            "999     185\n",
            "Name: prediction, dtype: int64\n",
            "\n",
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      7999\n",
            "           1       0.94      0.91      0.92        81\n",
            "           2       1.00      1.00      1.00        47\n",
            "           3       0.92      0.83      0.88        42\n",
            "           4       1.00      0.91      0.95        23\n",
            "\n",
            "    accuracy                           1.00      8192\n",
            "   macro avg       0.97      0.93      0.95      8192\n",
            "weighted avg       1.00      1.00      1.00      8192\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2   3   4\n",
            "Predicted                      \n",
            "0          7997   5   0   5   0\n",
            "1             1  74   0   2   2\n",
            "2             0   0  47   0   0\n",
            "3             1   2   0  35   0\n",
            "4             0   0   0   0  21\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.9518777594797191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "final_test = test.copy()\n",
        "final_test['prediction'] = model1.predict(final_test[x_train.columns])\n",
        "print(final_test['prediction'].value_counts())\n",
        "print('')\n",
        "\n",
        "one_class = final_test[final_test['prediction'] == 0]\n",
        "other_class = final_test[final_test['prediction'] != 0]\n",
        "other_class['prediction'] = model_1.predict(other_class[x_train.columns])\n",
        "final_test_df = pd.concat([one_class, other_class],axis=0)\n",
        "evaluate_model(final_test_df['target'], final_test_df['prediction'], [0,1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QILsy0E3skrk",
        "outputId": "d017b4ce-97f8-4503-ffb2-dc45647ae2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      2005\n",
            "999      44\n",
            "Name: prediction, dtype: int64\n",
            "\n",
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2001\n",
            "           1       0.58      0.55      0.56        20\n",
            "           2       1.00      1.00      1.00        12\n",
            "           3       0.55      0.60      0.57        10\n",
            "           4       1.00      0.33      0.50         6\n",
            "\n",
            "    accuracy                           0.99      2049\n",
            "   macro avg       0.82      0.70      0.73      2049\n",
            "weighted avg       0.99      0.99      0.99      2049\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2  3  4\n",
            "Predicted                    \n",
            "0          1996   7   0  0  2\n",
            "1             2  11   0  4  2\n",
            "2             0   0  12  0  0\n",
            "3             3   2   0  6  0\n",
            "4             0   0   0  0  2\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.7580895565139919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(b) Random Forest Classifier -  SMOTE sampling strategy**"
      ],
      "metadata": {
        "id": "0EQ4t2WY3DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onevsrest = train.copy()\n",
        "model2 = RandomForestClassifier(class_weight=\"balanced\")\n",
        "onevsrest['target'] = np.where(train['target'] == 0, 0, 999)\n",
        "x_train, x_val, y_train, y_val = split(onevsrest)\n",
        "index_values = list(onevsrest['target'].unique())\n",
        "\n",
        "X, y = sampling_strategy(\"SMOTE\")\n",
        "\n",
        "model2.fit(X, y)\n",
        "val_pred = model2.predict(x_val)\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVrA0A0da2FO",
        "outputId": "44338a5e-bbd6-44a3-d9d8-fe38ae11a5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1600\n",
            "         999       0.92      0.56      0.70        39\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.95      0.78      0.85      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True        0    999\n",
            "Predicted           \n",
            "0          1598   17\n",
            "999           2   22\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.7141592398954348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = RandomForestClassifier(class_weight=\"balanced\")\n",
        "df.dropna(axis=1,inplace=True)\n",
        "others = train[train['target'] != 0]\n",
        "index_values = list(others['target'].unique())\n",
        "\n",
        "x_train, x_val, y_train, y_val = split(others)\n",
        "\n",
        "X, y = sampling_strategy(\"SMOTE\")\n",
        "\n",
        "model_2.fit(X, y)\n",
        "val_pred = model_2.predict(x_val)\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu4Pt-eGct6y",
        "outputId": "22a9dc68-9af8-4f22-a69b-3edc8376140a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.81      0.72        16\n",
            "           2       1.00      1.00      1.00        10\n",
            "           3       0.57      0.50      0.53         8\n",
            "           4       1.00      0.40      0.57         5\n",
            "\n",
            "    accuracy                           0.74        39\n",
            "   macro avg       0.81      0.68      0.71        39\n",
            "weighted avg       0.77      0.74      0.74        39\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True        1   2  3  4\n",
            "Predicted              \n",
            "1          13   0  4  3\n",
            "2           0  10  0  0\n",
            "3           3   0  4  0\n",
            "4           0   0  0  2\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.6319982857856848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "final_train = train.copy()\n",
        "final_train['prediction'] = model2.predict(final_train[x_train.columns])\n",
        "print(final_train['prediction'].value_counts())\n",
        "print('')\n",
        "\n",
        "one_class = final_train[final_train['prediction'] == 0]\n",
        "other_class = final_train[final_train['prediction'] != 0]\n",
        "other_class['prediction'] = model_2.predict(other_class[x_train.columns])\n",
        "final_train_df = pd.concat([one_class, other_class],axis=0)\n",
        "evaluate_model(final_train_df['target'], final_train_df['prediction'], [0,1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HbleZRQsxNG",
        "outputId": "32350469-2536-4457-bd2e-70299b0ccffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      8014\n",
            "999     178\n",
            "Name: prediction, dtype: int64\n",
            "\n",
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      7999\n",
            "           1       0.90      0.88      0.89        81\n",
            "           2       1.00      1.00      1.00        47\n",
            "           3       0.91      0.74      0.82        42\n",
            "           4       1.00      0.78      0.88        23\n",
            "\n",
            "    accuracy                           1.00      8192\n",
            "   macro avg       0.96      0.88      0.92      8192\n",
            "weighted avg       1.00      1.00      1.00      8192\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2   3   4\n",
            "Predicted                      \n",
            "0          7997   7   0   8   2\n",
            "1             2  71   0   3   3\n",
            "2             0   0  47   0   0\n",
            "3             0   3   0  31   0\n",
            "4             0   0   0   0  18\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.9241847779836276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "final_test = test.copy()\n",
        "final_test['prediction'] = model2.predict(final_test[x_train.columns])\n",
        "print(final_test['prediction'].value_counts())\n",
        "print('')\n",
        "\n",
        "one_class = final_test[final_test['prediction'] == 0]\n",
        "other_class = final_test[final_test['prediction'] != 0]\n",
        "other_class['prediction'] = model_2.predict(other_class[x_train.columns])\n",
        "final_test_df = pd.concat([one_class, other_class],axis=0)\n",
        "evaluate_model(final_test_df['target'], final_test_df['prediction'], [0,1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay7K8Bg5s9BE",
        "outputId": "d346233f-140e-4f4c-f62e-ab775eaa5e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      2016\n",
            "999      33\n",
            "Name: prediction, dtype: int64\n",
            "\n",
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      2001\n",
            "           1       0.53      0.40      0.46        20\n",
            "           2       0.92      0.92      0.92        12\n",
            "           3       0.75      0.30      0.43        10\n",
            "           4       1.00      0.33      0.50         6\n",
            "\n",
            "    accuracy                           0.99      2049\n",
            "   macro avg       0.84      0.59      0.66      2049\n",
            "weighted avg       0.99      0.99      0.98      2049\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2  3  4\n",
            "Predicted                    \n",
            "0          1998  11   1  4  2\n",
            "1             2   8   0  3  2\n",
            "2             1   0  11  0  0\n",
            "3             0   1   0  3  0\n",
            "4             0   0   0  0  2\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.6744503284453877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(c) Catboost Classifier -  SMOTE sampling strategy**"
      ],
      "metadata": {
        "id": "eJWc1zQs3uAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onevsrest = train.copy()\n",
        "model3 = catboost.CatBoostClassifier(n_estimators=2500,random_state=0, learning_rate=0.03, verbose=100, early_stopping_rounds=50,auto_class_weights=\"Balanced\")\n",
        "onevsrest['target'] = np.where(train['target'] == 0, 0, 999)\n",
        "x_train, x_val, y_train, y_val = split(onevsrest)\n",
        "index_values = list(onevsrest['target'].unique())\n",
        "\n",
        "X, y = sampling_strategy(\"SMOTE\")\n",
        "\n",
        "model3.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model3.predict(x_val)\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8S7MCQVeBTS",
        "outputId": "728a7abf-ef20-4a93-af72-f1cdd9834486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1600\n",
            "         999       0.97      0.77      0.86        39\n",
            "\n",
            "    accuracy                           0.99      1639\n",
            "   macro avg       0.98      0.88      0.93      1639\n",
            "weighted avg       0.99      0.99      0.99      1639\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True        0    999\n",
            "Predicted           \n",
            "0          1599    9\n",
            "999           1   30\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.8599477523532122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = catboost.CatBoostClassifier(n_estimators=2500,random_state=0, learning_rate=0.03, verbose=100, early_stopping_rounds=50,auto_class_weights=\"Balanced\")\n",
        "df.dropna(axis=1,inplace=True)\n",
        "others = train[train['target'] != 0]\n",
        "index_values = list(others['target'].unique())\n",
        "\n",
        "x_train, x_val, y_train, y_val = split(others)\n",
        "\n",
        "X, y = sampling_strategy(\"SMOTE\")\n",
        "\n",
        "model_3.fit(X, y, eval_set=(x_val,y_val), verbose=0)\n",
        "val_pred = model_3.predict(x_val)\n",
        "evaluate_model(y_val, val_pred, index_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCpLu0X-eXgd",
        "outputId": "57faaa23-584c-445d-b1f5-a82b1afc83db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.72      0.81      0.76        16\n",
            "           2       1.00      1.00      1.00        10\n",
            "           3       0.56      0.62      0.59         8\n",
            "           4       1.00      0.40      0.57         5\n",
            "\n",
            "    accuracy                           0.77        39\n",
            "   macro avg       0.82      0.71      0.73        39\n",
            "weighted avg       0.79      0.77      0.76        39\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True        1   2  3  4\n",
            "Predicted              \n",
            "1          13   0  3  2\n",
            "2           0  10  0  0\n",
            "3           3   0  5  1\n",
            "4           0   0  0  2\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.6708132744529606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model - Catboost with SMOTE sampling strategy and One-vs-Rest dataset split"
      ],
      "metadata": {
        "id": "e1IHV9HTNIiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "final_train = train.copy()\n",
        "final_train['prediction'] = model3.predict(final_train[x_train.columns])\n",
        "print(final_train['prediction'].value_counts())\n",
        "print('')\n",
        "\n",
        "one_class = final_train[final_train['prediction'] == 0]\n",
        "other_class = final_train[final_train['prediction'] != 0]\n",
        "other_class['prediction'] = model_3.predict(other_class[x_train.columns])\n",
        "final_train_df = pd.concat([one_class, other_class],axis=0)\n",
        "evaluate_model(final_train_df['target'], final_train_df['prediction'], [0,1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9s5igDnJxDN",
        "outputId": "8ea3a41e-f1d3-4730-a7b1-d27bcfa6190c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      8007\n",
            "999     185\n",
            "Name: prediction, dtype: int64\n",
            "\n",
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      7999\n",
            "           1       0.93      0.93      0.93        81\n",
            "           2       1.00      0.94      0.97        47\n",
            "           3       0.90      0.86      0.88        42\n",
            "           4       1.00      0.87      0.93        23\n",
            "\n",
            "    accuracy                           1.00      8192\n",
            "   macro avg       0.96      0.92      0.94      8192\n",
            "weighted avg       1.00      1.00      1.00      8192\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2   3   4\n",
            "Predicted                      \n",
            "0          7998   3   3   3   0\n",
            "1             1  75   0   3   2\n",
            "2             0   0  44   0   0\n",
            "3             0   3   0  36   1\n",
            "4             0   0   0   0  20\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.9491902505974344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "final_test = test.copy()\n",
        "final_test['prediction'] = model3.predict(final_test[x_train.columns])\n",
        "print(final_test['prediction'].value_counts())\n",
        "print('')\n",
        "\n",
        "one_class = final_test[final_test['prediction'] == 0]\n",
        "other_class = final_test[final_test['prediction'] != 0]\n",
        "other_class['prediction'] = model_3.predict(other_class[x_train.columns])\n",
        "final_test_df = pd.concat([one_class, other_class],axis=0)\n",
        "evaluate_model(final_test_df['target'], final_test_df['prediction'], [0,1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzzBb78kJ-17",
        "outputId": "ab1618bd-7e25-4cf8-ff9c-c58d6387af58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      2007\n",
            "999      42\n",
            "Name: prediction, dtype: int64\n",
            "\n",
            "Classification report\n",
            "---------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2001\n",
            "           1       0.50      0.55      0.52        20\n",
            "           2       1.00      0.83      0.91        12\n",
            "           3       0.43      0.30      0.35        10\n",
            "           4       1.00      0.50      0.67         6\n",
            "\n",
            "    accuracy                           0.99      2049\n",
            "   macro avg       0.78      0.64      0.69      2049\n",
            "weighted avg       0.99      0.99      0.99      2049\n",
            "\n",
            "Confusion matrix\n",
            "---------------------------------------------------------------------------\n",
            "True          0   1   2  3  4\n",
            "Predicted                    \n",
            "0          1997   6   2  1  1\n",
            "1             3  11   0  6  2\n",
            "2             0   0  10  0  0\n",
            "3             1   3   0  3  0\n",
            "4             0   0   0  0  3\n",
            "\n",
            "Matthew Correlation coefficient\n",
            "---------------------------------------------------------------------------\n",
            "0.7198457511608933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance"
      ],
      "metadata": {
        "id": "NFuUW6ez1o1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(model3.feature_importances_, index=x_train.columns).nlargest(10).plot(kind='barh')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "aNngetLG2Kyv",
        "outputId": "e1dc67f9-fbe5-402d-c394-09d7eb959a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f58e6073490>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(model_3.feature_importances_, index=x_train.columns).nlargest(10).plot(kind='barh')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "6FwC6B1F1oU_",
        "outputId": "6b1db5fc-8583-4c93-e7d4-b6904c15c8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f58e8d04c50>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}